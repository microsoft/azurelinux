From a27e7d7149ff2c6cd9e232aeb48ce06579a3641e Mon Sep 17 00:00:00 2001
From: Sharath Srikanth Chellappa <sharathsr@microsoft.com>
Date: Mon, 11 Aug 2025 16:40:22 -0700
Subject: [PATCH] Adding support for NodePort-based control plane endpoint in a Kubevirt cluster

This patch is adding support for using a NodePort-based control plane endpoint in a Kubevirt cluster, 
along with wiring in configuration via Kubernetes secrets and environment variables.

Through this patch, if the service type is NodePort:
- Requires FABRIC_HOST_OVERRIDE env var to be set (host/IP to use externally).
- Calls externalLoadBalancer.GetNodePort() (new method) to get the port.
- If no port is assigned yet, marks the condition as LoadBalancerProvisioningFailed and requeues.
- Sets ControlPlaneEndpoint.Host to the host override and .Port to the NodePort.

---
 config/default/credentials.yaml               | 11 ++++
 config/default/kustomization.yaml             |  2 +
 config/default/manager_credentials_patch.yaml | 32 +++++++++
 controllers/kubevirtcluster_controller.go     | 19 +++++-
 pkg/loadbalancer/loadbalancer.go              | 14 ++++
 pkg/loadbalancer/loadbalancer_test.go         | 65 +++++++++++++++++++
 6 files changed, 142 insertions(+), 1 deletion(-)
 create mode 100644 config/default/credentials.yaml
 create mode 100644 config/default/manager_credentials_patch.yaml

diff --git a/config/default/credentials.yaml b/config/default/credentials.yaml
new file mode 100644
index 0000000..4ac5134
--- /dev/null
+++ b/config/default/credentials.yaml
@@ -0,0 +1,11 @@
+apiVersion: v1
+kind: Secret
+metadata:
+  name: manager-bootstrap-credentials
+  namespace: system
+type: Opaque
+data:
+  fabric-service-account-token: ${FABRIC_CLUSTER_SERVICE_ACCOUNT_TOKEN_B64}
+  fabric-control-endpoint: ${FABRIC_CLUSTER_CONTROL_ENDPOINT_B64}
+  fabric-cluster-ca-data: ${FABRIC_CLUSTER_CA_DATA_B64}
+  fabric-host-override: ${FABRIC_HOST_OVERRIDE_B64}
\ No newline at end of file
diff --git a/config/default/kustomization.yaml b/config/default/kustomization.yaml
index dc58c95..8b459dc 100644
--- a/config/default/kustomization.yaml
+++ b/config/default/kustomization.yaml
@@ -12,6 +12,7 @@ commonLabels:
 
 resources:
   - namespace.yaml
+  - credentials.yaml
 
 bases:
   - ../crd
@@ -22,6 +23,7 @@ bases:
 
 patchesStrategicMerge:
   # Provide customizable hook for make targets.
+  - manager_credentials_patch.yaml
   - manager_image_patch.yaml
   - manager_pull_policy.yaml
   # Enable webhook.
diff --git a/config/default/manager_credentials_patch.yaml b/config/default/manager_credentials_patch.yaml
new file mode 100644
index 0000000..7ebd3c3
--- /dev/null
+++ b/config/default/manager_credentials_patch.yaml
@@ -0,0 +1,32 @@
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: controller-manager
+  namespace: system
+spec:
+  template:
+    spec:
+      containers:
+      - name: manager
+        env:
+          - name: FABRIC_SERVICE_ACCOUNT_TOKEN
+            valueFrom:
+              secretKeyRef:
+                name: manager-bootstrap-credentials
+                key: fabric-service-account-token
+          - name: FABRIC_CLUSTER_CONTROL_ENDPOINT
+            valueFrom:
+              secretKeyRef:
+                name: manager-bootstrap-credentials
+                key: fabric-control-endpoint
+          - name: FABRIC_CLUSTER_CA_DATA
+            valueFrom:
+              secretKeyRef:
+                name: manager-bootstrap-credentials
+                key: fabric-cluster-ca-data
+          - name: FABRIC_HOST_OVERRIDE
+            valueFrom:
+              secretKeyRef:
+                name: manager-bootstrap-credentials
+                key: fabric-host-override
+                optional: true
diff --git a/controllers/kubevirtcluster_controller.go b/controllers/kubevirtcluster_controller.go
index e6b8924..41df42a 100644
--- a/controllers/kubevirtcluster_controller.go
+++ b/controllers/kubevirtcluster_controller.go
@@ -19,6 +19,7 @@ package controllers
 import (
 	gocontext "context"
 	"fmt"
+	"os"
 	"time"
 
 	"sigs.k8s.io/controller-runtime/pkg/builder"
@@ -188,7 +189,23 @@ func (r *KubevirtClusterReconciler) reconcileNormal(ctx *context.ClusterContext,
 			Port: 6443,
 		}
 
-		// Get Cluster IP if cluster Service Type is CusterIP
+		// Get NodePort if cluster Service Type is NodePort
+	} else if ctx.KubevirtCluster.Spec.ControlPlaneServiceTemplate.Spec.Type == "NodePort" {
+		if os.Getenv("FABRIC_HOST_OVERRIDE") == "" {
+			return ctrl.Result{}, errors.Errorf("NodePort selected but no FabricHostOverride specified")
+		}
+		if externalLoadBalancer.GetNodePort() == 0 {
+			conditions.MarkFalse(ctx.KubevirtCluster, infrav1.LoadBalancerAvailableCondition, infrav1.LoadBalancerProvisioningFailedReason, clusterv1.ConditionSeverityWarning, "NodePort not yet available")
+			return ctrl.Result{}, errors.Errorf("failed to get NodePort for the load balancer")
+		}
+
+		lbip4 := os.Getenv("FABRIC_HOST_OVERRIDE")
+		port := externalLoadBalancer.GetNodePort()
+		ctx.KubevirtCluster.Spec.ControlPlaneEndpoint = infrav1.APIEndpoint{
+			Host: lbip4,
+			Port: int(port),
+		}
+		// Get Cluster IP if cluster Service Type is ClusterIP
 	} else {
 		lbip4, err := externalLoadBalancer.IP(ctx)
 		if err != nil {
diff --git a/pkg/loadbalancer/loadbalancer.go b/pkg/loadbalancer/loadbalancer.go
index f440231..0cab0fd 100644
--- a/pkg/loadbalancer/loadbalancer.go
+++ b/pkg/loadbalancer/loadbalancer.go
@@ -121,6 +121,20 @@ func (l *LoadBalancer) Create(ctx *context.ClusterContext) error {
 	return nil
 }
 
+func (l *LoadBalancer) GetNodePort() int32 {
+	if !l.IsFound() {
+		return 0
+	}
+
+	for _, port := range l.service.Spec.Ports {
+		if port.Name == "ssh" {
+			return port.NodePort
+		}
+	}
+
+	return 0
+}
+
 // IP returns ip address of the load balancer
 func (l *LoadBalancer) IP(ctx *context.ClusterContext) (string, error) {
 	loadBalancer := &corev1.Service{}
diff --git a/pkg/loadbalancer/loadbalancer_test.go b/pkg/loadbalancer/loadbalancer_test.go
index b0f17e0..e35bbbe 100644
--- a/pkg/loadbalancer/loadbalancer_test.go
+++ b/pkg/loadbalancer/loadbalancer_test.go
@@ -23,6 +23,7 @@ import (
 	. "github.com/onsi/gomega"
 	corev1 "k8s.io/api/core/v1"
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/apimachinery/pkg/util/intstr"
 	ctrl "sigs.k8s.io/controller-runtime"
 	"sigs.k8s.io/controller-runtime/pkg/client"
 	"sigs.k8s.io/controller-runtime/pkg/client/fake"
@@ -39,6 +40,7 @@ var (
 	kubevirtCluster     = testing.NewKubevirtCluster(clusterName, kubevirtClusterName)
 	cluster             = testing.NewCluster(clusterName, kubevirtCluster)
 	loadBalancerService = newLoadBalancerService(clusterContext, kubevirtCluster)
+	nodePortService     = newNodePortService(clusterContext, kubevirtCluster)
 
 	clusterContext = &context.ClusterContext{
 		Logger:          ctrl.LoggerFrom(gocontext.TODO()).WithName("test"),
@@ -72,6 +74,11 @@ var _ = Describe("Load Balancer", func() {
 			Expect(lb.IsFound()).To(BeFalse())
 		})
 
+		It("should return nodePort as 0 for GetNodePort()", func() {
+			port := lb.GetNodePort()
+			Expect(port).To(Equal(int32(0)))
+		})
+
 		It("should return error for IP()", func() {
 			_, err := lb.IP(clusterContext)
 			Expect(err).To(HaveOccurred())
@@ -112,6 +119,36 @@ var _ = Describe("Load Balancer", func() {
 			Expect(err).To(HaveOccurred())
 		})
 	})
+
+	Context("when underlying service has been created already (NodePort service)", func() {
+		BeforeEach(func() {
+			objects := []client.Object{
+				cluster,
+				kubevirtCluster,
+				nodePortService,
+			}
+			fakeClient = fake.NewClientBuilder().WithScheme(testing.SetupScheme()).WithObjects(objects...).Build()
+		})
+
+		It("should initialize nodePort service without error", func() {
+			lb, err = loadbalancer.NewLoadBalancer(clusterContext, fakeClient, "")
+			Expect(err).NotTo(HaveOccurred())
+		})
+
+		It("should return true for isFound()", func() {
+			Expect(lb.IsFound()).To(BeTrue())
+		})
+
+		It("should return non-empty nodePort", func() {
+			port := lb.GetNodePort()
+			Expect(port).ToNot(BeZero())
+		})
+
+		It("should NOT create a new load balancer", func() {
+			err = lb.Create(clusterContext)
+			Expect(err).To(HaveOccurred())
+		})
+	})
 })
 
 func newLoadBalancerService(ctx *context.ClusterContext, kubevirtCluster *infrav1.KubevirtCluster) *corev1.Service {
@@ -131,3 +168,31 @@ func newLoadBalancerService(ctx *context.ClusterContext, kubevirtCluster *infrav
 		Spec: corev1.ServiceSpec{ClusterIP: "1.1.1.1"},
 	}
 }
+
+func newNodePortService(ctx *context.ClusterContext, kubevirtCluster *infrav1.KubevirtCluster) *corev1.Service {
+	return &corev1.Service{
+		TypeMeta: metav1.TypeMeta{},
+		ObjectMeta: metav1.ObjectMeta{
+			Name: clusterName + "-lb",
+			OwnerReferences: []metav1.OwnerReference{
+				{
+					APIVersion: kubevirtCluster.APIVersion,
+					Kind:       kubevirtCluster.Kind,
+					Name:       kubevirtCluster.Name,
+					UID:        kubevirtCluster.UID,
+				},
+			},
+		},
+		Spec: corev1.ServiceSpec{
+			Ports: []corev1.ServicePort{
+				{
+					Name:       "ssh",
+					Port:       22,
+					NodePort:   30000,
+					Protocol:   corev1.ProtocolTCP,
+					TargetPort: intstr.FromInt(22),
+				},
+			},
+		},
+	}
+}
-- 
2.49.0

