From 2101fbd9eed4370e87a31dc099085af6e1d5718e Mon Sep 17 00:00:00 2001
From: AllSpark <allspark@microsoft.com>
Date: Wed, 10 Dec 2025 17:21:46 +0000
Subject: [PATCH] Backport: streaming decompression safety and decoder API
 adjustments; docs wording update for streaming responses. Includes max_length
 handling and has_unconsumed_tail, Brotli warning using DependencyWarning.

Signed-off-by: Azure Linux Security Servicing Account <azurelinux-security@microsoft.com>
Upstream-reference: AI Backport of https://github.com/urllib3/urllib3/commit/24d7b67eac89f94e11003424bcf0d8f7b72222a8.patch
---
 docs/advanced-usage.rst |   3 +-
 docs/user-guide.rst     |   4 +-
 src/urllib3/response.py | 169 +++++++++++++++++++++++++++++++++++-----
 3 files changed, 153 insertions(+), 23 deletions(-)

diff --git a/docs/advanced-usage.rst b/docs/advanced-usage.rst
index d38e719..d2b4c40 100644
--- a/docs/advanced-usage.rst
+++ b/docs/advanced-usage.rst
@@ -57,7 +57,8 @@ When using ``preload_content=True`` (the default setting) the
 response body will be read immediately into memory and the HTTP connection
 will be released back into the pool without manual intervention.
 
-However, when dealing with large responses it's often better to stream the response
+However, when dealing with responses of large or unknown length,
+it's often better to stream the response
 content using ``preload_content=False``. Setting ``preload_content`` to ``False`` means
 that urllib3 will only read from the socket when data is requested.
 
diff --git a/docs/user-guide.rst b/docs/user-guide.rst
index abd5323..bb16b9a 100644
--- a/docs/user-guide.rst
+++ b/docs/user-guide.rst
@@ -99,8 +99,8 @@ to a byte string representing the response content:
     >>> r.data
     b'\xaa\xa5H?\x95\xe9\x9b\x11'
 
-.. note:: For larger responses, it's sometimes better to :ref:`stream <stream>`
-    the response.
+.. note:: For responses of large or unknown length, it's sometimes better to
+    :ref:`stream <stream>` the response.
 
 Using io Wrappers with Response Content
 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
diff --git a/src/urllib3/response.py b/src/urllib3/response.py
index bcb1d80..bed100c 100644
--- a/src/urllib3/response.py
+++ b/src/urllib3/response.py
@@ -23,6 +23,7 @@ from .connection import BaseSSLError, HTTPException
 from .exceptions import (
     BodyNotHttplibCompatible,
     DecodeError,
+    DependencyWarning,
     HTTPError,
     IncompleteRead,
     InvalidChunkLength,
@@ -41,34 +42,66 @@ log = logging.getLogger(__name__)
 class DeflateDecoder(object):
     def __init__(self):
         self._first_try = True
+        self._first_try_data = b""
+        self._unfed_data = b""
         self._data = b""
         self._obj = zlib.decompressobj()
 
     def __getattr__(self, name):
         return getattr(self._obj, name)
 
-    def decompress(self, data):
-        if not data:
+    def decompress(self, data, max_length=-1):
+        data = self._unfed_data + data
+        self._unfed_data = b""
+        if not data and not getattr(self._obj, "unconsumed_tail", b""):
             return data
+        original_max_length = max_length
+        if original_max_length < 0:
+            max_length = 0
+        elif original_max_length == 0:
+            # We should not pass 0 to the zlib decompressor because 0 is
+            # the default value that will make zlib decompress without a
+            # length limit.
+            # Data should be stored for subsequent calls.
+            self._unfed_data = data
+            return b""
 
+        # Subsequent calls always reuse `self._obj`. zlib requires
+        # passing the unconsumed tail if decompression is to continue.
         if not self._first_try:
-            return self._obj.decompress(data)
+            return self._obj.decompress(
+                getattr(self._obj, "unconsumed_tail", b"") + data, max_length=max_length
+            )
 
+        # First call tries with RFC 1950 ZLIB format.
+        self._first_try_data += data
         self._data += data
         try:
-            decompressed = self._obj.decompress(data)
+            decompressed = self._obj.decompress(data, max_length=max_length)
             if decompressed:
                 self._first_try = False
+                self._first_try_data = b""
                 self._data = None
             return decompressed
+        # On failure, it falls back to RFC 1951 DEFLATE format.
         except zlib.error:
             self._first_try = False
             self._obj = zlib.decompressobj(-zlib.MAX_WBITS)
             try:
-                return self.decompress(self._data)
+                return self.decompress(self._first_try_data, max_length=original_max_length)
             finally:
+                self._first_try_data = b""
                 self._data = None
 
+    @property
+    def has_unconsumed_tail(self):
+        return bool(self._unfed_data) or (
+            bool(getattr(self._obj, "unconsumed_tail", b"")) and not self._first_try
+        )
+
+    def flush(self):
+        return self._obj.flush()
+
 
 class GzipDecoderState(object):
 
@@ -81,30 +114,64 @@ class GzipDecoder(object):
     def __init__(self):
         self._obj = zlib.decompressobj(16 + zlib.MAX_WBITS)
         self._state = GzipDecoderState.FIRST_MEMBER
+        self._unconsumed_tail = b""
 
     def __getattr__(self, name):
         return getattr(self._obj, name)
 
-    def decompress(self, data):
+    def decompress(self, data, max_length=-1):
         ret = bytearray()
-        if self._state == GzipDecoderState.SWALLOW_DATA or not data:
+        if self._state == GzipDecoderState.SWALLOW_DATA:
             return bytes(ret)
+
+        if max_length == 0:
+            # We should not pass 0 to the zlib decompressor because 0 is
+            # the default value that will make zlib decompress without a
+            # length limit.
+            # Data should be stored for subsequent calls.
+            self._unconsumed_tail += data
+            return b""
+
+        # zlib requires passing the unconsumed tail to the subsequent
+        # call if decompression is to continue.
+        data = self._unconsumed_tail + data
+        if not data and self._obj.eof:
+            return bytes(ret)
+
         while True:
             try:
-                ret += self._obj.decompress(data)
+                ret += self._obj.decompress(
+                    data, max_length=max(max_length - len(ret), 0)
+                )
             except zlib.error:
                 previous_state = self._state
                 # Ignore data after the first error
                 self._state = GzipDecoderState.SWALLOW_DATA
+                self._unconsumed_tail = b""
                 if previous_state == GzipDecoderState.OTHER_MEMBERS:
                     # Allow trailing garbage acceptable in other gzip clients
                     return bytes(ret)
                 raise
-            data = self._obj.unused_data
+
+            self._unconsumed_tail = data = (
+                getattr(self._obj, "unconsumed_tail", b"") or self._obj.unused_data
+            )
+            if max_length > 0 and len(ret) >= max_length:
+                break
+
             if not data:
                 return bytes(ret)
-            self._state = GzipDecoderState.OTHER_MEMBERS
-            self._obj = zlib.decompressobj(16 + zlib.MAX_WBITS)
+            # When the end of a gzip member is reached, a new decompressor
+            # must be created for unused (possibly future) data.
+            if getattr(self._obj, "eof", False):
+                self._state = GzipDecoderState.OTHER_MEMBERS
+                self._obj = zlib.decompressobj(16 + zlib.MAX_WBITS)
+
+        return bytes(ret)
+
+    @property
+    def has_unconsumed_tail(self):
+        return bool(self._unconsumed_tail)
 
 
 if brotli is not None:
@@ -116,9 +183,35 @@ if brotli is not None:
         def __init__(self):
             self._obj = brotli.Decompressor()
             if hasattr(self._obj, "decompress"):
-                self.decompress = self._obj.decompress
+                self._decompress = self._obj.decompress
             else:
-                self.decompress = self._obj.process
+                self._decompress = self._obj.process
+
+        # Requires Brotli >= 1.2.0 for `output_buffer_limit`.
+        def _decompress(self, data, output_buffer_limit=-1):
+            raise NotImplementedError()
+
+        def decompress(self, data, max_length=-1):
+            try:
+                if max_length > 0:
+                    return self._decompress(data, output_buffer_limit=max_length)
+                else:
+                    return self._decompress(data)
+            except TypeError:
+                # Fallback for Brotli/brotlicffi/brotlipy versions without
+                # the `output_buffer_limit` parameter.
+                warnings.warn(
+                    "Brotli >= 1.2.0 is required to prevent decompression bombs.",
+                    DependencyWarning,
+                )
+                return self._decompress(data)
+
+        @property
+        def has_unconsumed_tail(self):
+            try:
+                return not self._obj.can_accept_more_data()
+            except AttributeError:
+                return False
 
         def flush(self):
             if hasattr(self._obj, "flush"):
@@ -151,10 +244,35 @@ class MultiDecoder(object):
     def flush(self):
         return self._decoders[0].flush()
 
-    def decompress(self, data):
-        for d in reversed(self._decoders):
-            data = d.decompress(data)
-        return data
+    def decompress(self, data, max_length=-1):
+        if max_length <= 0:
+            for d in reversed(self._decoders):
+                data = d.decompress(data)
+            return data
+
+        ret = bytearray()
+        # Every while loop iteration goes through all decoders once.
+        # It exits when enough data is read or no more data can be read.
+        # It is possible that the while loop iteration does not produce
+        # any data because we retrieve up to `max_length` from every
+        # decoder, and the amount of bytes may be insufficient for the
+        # next decoder to produce enough/any output.
+        while True:
+            any_data = False
+            for d in reversed(self._decoders):
+                data = d.decompress(data, max_length=max_length - len(ret))
+                if data:
+                    any_data = True
+                # We should not break when no data is returned because
+                # next decoders may produce data even with empty input.
+            ret += data
+            if not any_data or len(ret) >= max_length:
+                return bytes(ret)
+            data = b""
+
+    @property
+    def has_unconsumed_tail(self):
+        return any(getattr(d, "has_unconsumed_tail", False) for d in self._decoders)
 
 
 def _get_decoder(mode):
@@ -405,16 +523,22 @@ class HTTPResponse(io.IOBase):
     if brotli is not None:
         DECODER_ERROR_CLASSES += (brotli.error,)
 
-    def _decode(self, data, decode_content, flush_decoder):
+    def _decode(self, data, decode_content, flush_decoder, max_length=None):
         """
         Decode the data passed in and potentially flush the decoder.
         """
         if not decode_content:
             return data
 
+        # Determine the maximum length to decode in this call
+        if max_length is None or flush_decoder:
+            max_len = -1
+        else:
+            max_len = max_length
+
         try:
             if self._decoder:
-                data = self._decoder.decompress(data)
+                data = self._decoder.decompress(data, max_length=max_len)
         except self.DECODER_ERROR_CLASSES as e:
             content_encoding = self.headers.get("content-encoding", "").lower()
             raise DecodeError(
@@ -607,7 +731,12 @@ class HTTPResponse(io.IOBase):
             if self.length_remaining is not None:
                 self.length_remaining -= len(data)
 
-            data = self._decode(data, decode_content, flush_decoder)
+            data = self._decode(
+                data,
+                decode_content,
+                flush_decoder,
+                max_length=(amt if amt is not None else None),
+            )
 
             if cache_content:
                 self._body = data
-- 
2.45.4

