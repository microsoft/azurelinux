From cef4c4308729099f4d2fc11bfd5f5132cc6c0225 Mon Sep 17 00:00:00 2001
From: AllSpark <allspark@microsoft.com>
Date: Wed, 28 Jan 2026 18:21:32 +0000
Subject: [PATCH] override SWALR.state_dict and load_state_dict; add
 _set_anneal_func and use in __init__

Signed-off-by: Azure Linux Security Servicing Account <azurelinux-security@microsoft.com>
Upstream-reference: AI Backport of https://github.com/pytorch/pytorch/commit/167ad09be5af5c52666759412a3804068c6955d1.patch
---
 torch/optim/swa_utils.py | 39 +++++++++++++++++++++++++++++++++++----
 1 file changed, 35 insertions(+), 4 deletions(-)

diff --git a/torch/optim/swa_utils.py b/torch/optim/swa_utils.py
index 90b3f159..e792f727 100644
--- a/torch/optim/swa_utils.py
+++ b/torch/optim/swa_utils.py
@@ -7,6 +7,8 @@ import torch
 from torch.nn import Module
 from torch.optim.lr_scheduler import LRScheduler
 from torch.utils._foreach_utils import _get_foreach_kernels_supported_devices
+from typing_extensions import override
+
 
 __all__ = [
     'AveragedModel',
@@ -326,10 +328,7 @@ class SWALR(LRScheduler):
         if anneal_strategy not in ['cos', 'linear']:
             raise ValueError("anneal_strategy must by one of 'cos' or 'linear', "
                              f"instead got {anneal_strategy}")
-        elif anneal_strategy == 'cos':
-            self.anneal_func = self._cosine_anneal
-        elif anneal_strategy == 'linear':
-            self.anneal_func = self._linear_anneal
+        self._set_anneal_func(anneal_strategy)
         if not isinstance(anneal_epochs, int) or anneal_epochs < 0:
             raise ValueError(f"anneal_epochs must be equal or greater than 0, got {anneal_epochs}")
         self.anneal_epochs = anneal_epochs
@@ -375,3 +374,35 @@ class SWALR(LRScheduler):
         alpha = self.anneal_func(t)
         return [group['swa_lr'] * alpha + lr * (1 - alpha)
                 for group, lr in zip(self.optimizer.param_groups, prev_lrs)]
+
+
+    def _set_anneal_func(self, anneal_strategy):
+        self._anneal_strategy = anneal_strategy
+        if anneal_strategy == 'cos':
+            self.anneal_func = self._cosine_anneal
+        else:
+            self.anneal_func = self._linear_anneal
+
+    @override
+    def state_dict(self):
+        """Return the state of the scheduler as a :class:`dict`.
+
+        It contains an entry for every variable in self.__dict__ which
+        is not the optimizer or anneal_func.
+        """
+        return {
+            key: value
+            for key, value in self.__dict__.items()
+            if key not in ("optimizer", "anneal_func")
+        }
+
+    @override
+    def load_state_dict(self, state_dict):
+        """Load the scheduler's state.
+
+        Args:
+            state_dict (dict): scheduler state. Should be an object returned
+                from a call to :meth:`state_dict`.
+        """
+        self.__dict__.update(state_dict)
+        self._set_anneal_func(self._anneal_strategy)
-- 
2.45.4

