From ccc348378ad708ccda85c4e87da5051547bdddfd Mon Sep 17 00:00:00 2001
From: AllSpark <allspark@microsoft.com>
Date: Wed, 1 Oct 2025 14:07:52 +0000
Subject: [PATCH] Disable parallel reduction under simd_vec loop; add test for
 parallel reduction vectorization; remove CPU skip in flex attention test
 (ghstack-poisoned).

Signed-off-by: Azure Linux Security Servicing Account <azurelinux-security@microsoft.com>
Upstream-reference: AI Backport of https://patch-diff.githubusercontent.com/raw/pytorch/pytorch/pull/143635.patch
---
 test/inductor/test_cpu_repro.py | 28 ++++++++++++++++++++++++++++
 torch/_inductor/codegen/cpp.py  | 25 +++++++++++++++++++++++++
 2 files changed, 53 insertions(+)

diff --git a/test/inductor/test_cpu_repro.py b/test/inductor/test_cpu_repro.py
index 925c0f62..7983904b 100644
--- a/test/inductor/test_cpu_repro.py
+++ b/test/inductor/test_cpu_repro.py
@@ -1013,6 +1013,34 @@ class CPUReproTests(TestCase):
                 x, 1.0, 0, 0, 255, torch.uint8
             )
             x = torch.transpose(x, 1, 2).contiguous()
+
+    def test_parallel_reduction_vectorization(self):
+        # Fix issue: https://github.com/pytorch/pytorch/issues/151523
+        class Model(torch.nn.Module):
+            def __init__(self):
+                super().__init__()
+                self.conv = torch.nn.Conv2d(
+                    in_channels=3,
+                    out_channels=16,
+                    kernel_size=(1, 7),
+                    stride=(2, 1),
+                    padding=0,
+                )
+
+            def forward(self, x, weight):
+                x = self.conv(x)
+                x = F.hardshrink(x, lambd=0)
+                x = x.view(x.size(0), -1)
+                x = torch.mv(weight, x[0])
+                return x
+
+        mod = Model().eval()
+        x = torch.randn(2, 3, 127, 255)
+        weight = torch.randn(10, 254976)
+        # Use same criterion as test_inplace_squeeze_needed
+        # for parallel reduction.
+        self.common(mod, (x, weight), atol=5e-1, rtol=5e-1)
+
             x = x.view(batchsize, num_channels, height, width)
             return x
 
diff --git a/torch/_inductor/codegen/cpp.py b/torch/_inductor/codegen/cpp.py
index b94ede02..45cad252 100644
--- a/torch/_inductor/codegen/cpp.py
+++ b/torch/_inductor/codegen/cpp.py
@@ -3346,6 +3346,24 @@ class LoopNestWithSplit:
             loops += loop.get_loops_at(depth)
         return loops
 
+
+    def get_simd_vec_depth(self):
+        """
+        Return the first loop depth which is marked as simd_vec.
+        Depth is counted from the outermost loop at 0. If no simd_vec loop
+        is found or the structure branches, return None.
+        """
+        assert self.root is not None
+        loops = self.root
+        depth = 0
+        while len(loops) == 1:
+            loop = loops[0]
+            if getattr(loop, "simd_vec", False):
+                return depth
+            loops = loop.inner
+            depth += 1
+        return None
+
     @cache_on_self
     def max_parallel_depth(self):
         """
@@ -3357,12 +3375,19 @@ class LoopNestWithSplit:
         max_depth = 0
         assert self.root is not None
         loops = self.root
+        simd_vec_depth = self.get_simd_vec_depth()
         if len(loops) > 1:
             return 1
         is_reduction = loops[0].is_reduction() if loops else False
         while len(loops) == 1 and loops[0].is_reduction() == is_reduction:
             max_depth += 1
             loops = loops[0].inner
+        # If parallelism would be placed under the simd_vec loop on a reduction,
+        # disable it by reducing max_depth by 1.
+        if simd_vec_depth is not None and max_depth > simd_vec_depth:
+            target_loops = self.get_loops_at(max_depth - 1)
+            if len(target_loops) == 1 and target_loops[0].is_reduction():
+                max_depth -= 1
         return max_depth
 
     def is_reduction_only(self):
-- 
2.45.4

