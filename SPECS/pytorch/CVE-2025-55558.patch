From 63bac37d8a346e642dd4ae4be98e3c3e30a6f128 Mon Sep 17 00:00:00 2001
From: AllSpark <allspark@microsoft.com>
Date: Wed, 1 Oct 2025 13:16:42 +0000
Subject: [PATCH] Backport: disable parallel reduction under vec loop; add
 test; unskip flex_attention test per original patch

Signed-off-by: Azure Linux Security Servicing Account <azurelinux-security@microsoft.com>
Upstream-reference: AI Backport of https://patch-diff.githubusercontent.com/raw/pytorch/pytorch/pull/143635.patch
---
 test/inductor/test_cpu_repro.py | 28 ++++++++++++++++++++++++++++
 torch/_inductor/codegen/cpp.py  | 22 ++++++++++++++++++++++
 2 files changed, 50 insertions(+)

diff --git a/test/inductor/test_cpu_repro.py b/test/inductor/test_cpu_repro.py
index 925c0f62..966d1a20 100644
--- a/test/inductor/test_cpu_repro.py
+++ b/test/inductor/test_cpu_repro.py
@@ -601,6 +601,34 @@ class CPUReproTests(TestCase):
         # aten parallel.
         self.common(fn, (v,), atol=5e-1, rtol=5e-1)
 
+
+    def test_parallel_reduction_vectorization(self):
+        # Fix issue: https://github.com/pytorch/pytorch/issues/151523
+        class Model(torch.nn.Module):
+            def __init__(self):
+                super().__init__()
+                self.conv = torch.nn.Conv2d(
+                    in_channels=3,
+                    out_channels=16,
+                    kernel_size=(1, 7),
+                    stride=(2, 1),
+                    padding=0,
+                )
+
+            def forward(self, x, weight):
+                x = self.conv(x)
+                x = F.hardshrink(x, lambd=0)
+                x = x.view(x.size(0), -1)
+                x = torch.mv(weight, x[0])
+                return x
+
+        mod = Model().eval()
+        x = torch.randn(2, 3, 127, 255)
+        weight = torch.randn(10, 254976)
+        # Use same criterion as test_inplace_squeeze_needed
+        # for parallel reduction.
+        self.common(mod, (x, weight), atol=5e-1, rtol=5e-1)
+
     def test_cat_mul(self):
         # https://github.com/pytorch/pytorch/issues/93365
         def fn(p0, p1):
diff --git a/torch/_inductor/codegen/cpp.py b/torch/_inductor/codegen/cpp.py
index b94ede02..9724c112 100644
--- a/torch/_inductor/codegen/cpp.py
+++ b/torch/_inductor/codegen/cpp.py
@@ -3359,8 +3359,30 @@ class LoopNestWithSplit:
         loops = self.root
         if len(loops) > 1:
             return 1
+
+        def get_simd_vec_depth(loops):
+            # Return the first loop level which is simd_vec
+            if not loops:
+                return None
+            depth = 0
+            loop = loops[0]
+            while loop is not None:
+                if loop.simd_vec:
+                    return depth
+                if loop.inner:
+                    loop = loop.inner[0]
+                    depth += 1
+                else:
+                    break
+            return None
+
+        simd_vec_depth = get_simd_vec_depth(loops)
+
         is_reduction = loops[0].is_reduction() if loops else False
         while len(loops) == 1 and loops[0].is_reduction() == is_reduction:
+            # Disable parallel reduction under the vec loop
+            if simd_vec_depth is not None and max_depth > simd_vec_depth and loops[0].is_reduction():
+                break
             max_depth += 1
             loops = loops[0].inner
         return max_depth
-- 
2.45.4

