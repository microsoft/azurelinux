From 5d84ff625734ae2bb4f3345bd6a534e9f93a4946 Mon Sep 17 00:00:00 2001
From: AllSpark <allspark@microsoft.com>
Date: Wed, 1 Oct 2025 12:45:08 +0000
Subject: [PATCH] Backport: graph break for sparse Tensor wrapping; add import
 of is_sparse_any from gradcheck

Signed-off-by: Azure Linux Security Servicing Account <azurelinux-security@microsoft.com>
Upstream-reference: AI Backport of https://patch-diff.githubusercontent.com/raw/pytorch/pytorch/pull/151897.patch
---
 torch/_dynamo/variables/builtin.py | 11 +++++++++++
 1 file changed, 11 insertions(+)

diff --git a/torch/_dynamo/variables/builtin.py b/torch/_dynamo/variables/builtin.py
index 91ea1114..151b22c8 100644
--- a/torch/_dynamo/variables/builtin.py
+++ b/torch/_dynamo/variables/builtin.py
@@ -10,6 +10,7 @@ from typing import Dict, List
 import torch
 from torch import sym_float, sym_int
 
+from torch.autograd.gradcheck import _is_sparse_any_tensor as is_sparse_any
 from .. import config, variables
 from ..allowed_functions import is_allowed
 from ..exc import unimplemented, Unsupported
@@ -906,6 +907,16 @@ class BuiltinVariable(VariableTracker):
         if not name_var.is_python_constant():
             unimplemented("non-const getattr() name")
 
+        # Graph break on unsupported sparse Tensor wrapping
+        if isinstance(obj, variables.TensorVariable):
+            fake_val = obj.proxy.node.meta.get("example_value", None)
+            if isinstance(fake_val, torch.Tensor) and is_sparse_any(fake_val):
+                if (not getattr(tx, "export", False)) or (
+                    not getattr(config, "capture_sparse_compute", False)
+                ):
+                    # torch.compile does not support sparse Tensors without capture flag
+                    unimplemented("Attempted to wrap sparse Tensor")
+
         if tx.output.side_effects.is_attribute_mutation(obj):
             try:
                 # re-read a pending side effect?
-- 
2.45.4

