From 5d84ff625734ae2bb4f3345bd6a534e9f93a4946 Mon Sep 17 00:00:00 2001
From: AllSpark <allspark@microsoft.com>
Date: Wed, 1 Oct 2025 12:45:08 +0000
Subject: [PATCH] Backport: graph break for sparse Tensor wrapping; add import
 of is_sparse_any from gradcheck

Signed-off-by: Azure Linux Security Servicing Account <azurelinux-security@microsoft.com>
Upstream-reference: AI Backport of https://patch-diff.githubusercontent.com/raw/pytorch/pytorch/pull/151897.patch

---
 torch/_dynamo/variables/builtin.py | 10 +++++++++-
 1 file changed, 9 insertions(+), 1 deletion(-)

diff --git a/torch/_dynamo/variables/builtin.py b/torch/_dynamo/variables/builtin.py
index 91ea1114..c1a2c2a2 100644
--- a/torch/_dynamo/variables/builtin.py
+++ b/torch/_dynamo/variables/builtin.py
@@ -9,7 +9,7 @@ from typing import Dict, List
 
 import torch
 from torch import sym_float, sym_int
-
+from torch.autograd.gradcheck import _is_sparse_any_tensor as is_sparse_any
 from .. import config, variables
 from ..allowed_functions import is_allowed
 from ..exc import unimplemented, Unsupported
@@ -953,6 +953,14 @@ class BuiltinVariable(VariableTracker):
                 variables.UserDefinedObjectVariable,
             ),
         ):
+            if isinstance(obj, variables.TensorVariable):
+                fake_val = obj.proxy.node.meta.get("example_value", None)
+                if isinstance(fake_val, torch.Tensor) and is_sparse_any(fake_val):
+                    if (not getattr(tx, "export", False)) or (
+                        not getattr(config, "capture_sparse_compute", False)
+                    ):
+                        # torch.compile does not support sparse Tensors without capture flag
+                        unimplemented("Attempted to wrap sparse Tensor")
             try:
                 return (
                     obj.var_getattr(tx, name).clone(source=source).add_options(options)
-- 
2.43.0

