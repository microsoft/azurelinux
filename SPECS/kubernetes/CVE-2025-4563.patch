From cc76924c591da0c0a7cfcd9314dba1ba5029c68b Mon Sep 17 00:00:00 2001
From: Rohit Rawat <rohitrawat@microsoft.com>
Date: Mon, 30 Jun 2025 11:37:54 +0000
Subject: [PATCH] Merged PR 23726: Send tar to swe-agent

#### AI description  (iteration 1)
#### PR Classification
This PR implements a new feature by integrating tar file submission to swe-agent with enhanced patch backporting support.

#### PR Summary
The changes refactor the patch preparation workflow to support conditional backporting, enable tar-based patch delivery to swe-agent, and improve error handling and status tracking.
- **`patch_single_package.py`**: Introduces a `try_backport` flag and refactors the patch preparation logic to conditionally obtain backported patches from an auxiliary directory.
- **`prepare_patch.py`**: Refactors patch application functions to combine regular and backport workflows, update git commands, and generate tar archives appropriately.
- **`utils/file_util.py`**: Adds a new utility module to manage auxiliary directories, status tracking, and file operations for backport candidates.
- **Pipeline YAML (`autosec_azl_e2e.yml`)**: Updates tasks to conditionally attempt backporting, publish additional artifacts, and adjust parameter handling.
- **`db_util.py` and `swe_agent.py`**: Enhance database connection retry mechanisms and adjust swe-agent integration to handle tar submissions effectively.
<!-- GitOpsUserAgent=GitOps.Apps.Server.pullrequestcopilot -->
---
 ai_utils.py                    |   8 +-
 api_utils.py                   |   7 +-
 autosec_azl_e2e.yml            | 117 +++++++--
 db_util.py                     |  39 ++-
 patch_all_package.py           |  57 ++++-
 patch_single_package.py        | 150 +++++++----
 prepare_patch.py               | 439 +++++++++++++++++++--------------
 publishing.py => publish_pr.py | 149 ++++++++---
 summary.py                     |  76 ++++++
 swe_agent.py                   |  70 ++++--
 utils/__init__.py              |   0
 utils/file_util.py             | 124 ++++++++++
 12 files changed, 905 insertions(+), 331 deletions(-)
 rename publishing.py => publish_pr.py (57%)
 create mode 100644 summary.py
 create mode 100644 utils/__init__.py
 create mode 100644 utils/file_util.py

diff --git a/ai_utils.py b/ai_utils.py
index 7dcb999..ead8d4d 100644
--- a/ai_utils.py
+++ b/ai_utils.py
@@ -12,12 +12,18 @@ class CveDetails:
     """
 
     def __init__(
-        self, cve_id: str, package_name: str, patch_link: str, working_dir: str
+        self,
+        azure_linux_version: str,
+        cve_id: str,
+        package_name: str,
+        patch_link: str,
+        working_dir: str,
     ):
         self.cve_id = cve_id
         self.package_name = package_name
         self.patch_link = patch_link
         self.working_dir = working_dir
+        self.azure_linux_version = azure_linux_version
         self.spec_file_path = (
             f"{working_dir}/azurelinux/SPECS/{package_name}/{package_name}.spec"
         )
diff --git a/api_utils.py b/api_utils.py
index a63e33d..d198730 100644
--- a/api_utils.py
+++ b/api_utils.py
@@ -95,14 +95,17 @@ def getJson(package: str, cve_id: Optional[str] = "") -> dict:
     return final_res
 
 
-def get_ai_final_verdict(package: str, cve_id: str):
+def get_ai_final_verdict(azure_linux_version: str, package: str, cve_id: str):
     """
     Returns patch in AI final verdict from Astrolabe.
     """
     url = f"https://astrolabeapi.azurewebsites.net/api/cves/getFullCVEByIdForOfficialBranches/{cve_id}"
     response = get_response_json(url)
     for cveObj in response["cves"]:
-        if cveObj["packageName"] == package:
+        if (
+            cveObj["packageName"] == package
+            and cveObj["azureLinuxBranch"] == azure_linux_version
+        ):
             return cveObj.get("patchLinkIfAvailable", None)
 
 
diff --git a/autosec_azl_e2e.yml b/autosec_azl_e2e.yml
index eb04726..ebcbbc0 100644
--- a/autosec_azl_e2e.yml
+++ b/autosec_azl_e2e.yml
@@ -25,12 +25,27 @@ parameters:
     displayName: "(optional) List of space separated patch URL, each URL should be in single quotes"
     default: "Not-Applicable"
 
+  - name: attemptBackport
+    type: boolean
+    displayName: "Attempt backporting patches"
+    default: true
+    values:
+      - true
+      - false
+
 jobs:
 - job: Autosec_Pipeline
   displayName: 'AutoSec'
   pool: autosec_alz30_pool 
   timeoutInMinutes: 1000
   steps:
+  - bash: |
+      if [ "${{ parameters.packageName }}" != "Not-Applicable" ]; then
+        BUILD_NB="${{ parameters.azureLinuxVersion }} - ${{ parameters.packageName }}"
+        echo "Setting build number to: $BUILD_NB"
+        echo "##vso[build.updatebuildnumber]$BUILD_NB"
+      fi
+    displayName: Set Build number for single package
   - bash: |
       function install_tdnf_package() {
         local package_name=$1
@@ -53,7 +68,7 @@ jobs:
     displayName: 'Install Dependencies'
 
   - task: AzureCLI@2
-    displayName: "Autosec Fixing CVEs"
+    displayName: "Patch without backporting"
     continueOnError: true
     inputs:
       azureSubscription: autosec-astrolabe-interface
@@ -67,30 +82,83 @@ jobs:
         git config --global user.email "$(GITHUB_EMAIL)"
         git config --global github.token "$(GITHUB_TOKEN)"
         
-        echo "Starting Autosec tool"
+        echo "Starting Autosec tool without backporting"
         working_dir=$(pwd)/autosec
 
         mkdir -p $working_dir
         echo "Working directory: $working_dir"
 
-        # if packageName is not empty, use it to patch
-        if [ "${{ parameters.packageName }}" != "Not-Applicable" ]; then
-          echo "Patching package: ${{ parameters.packageName }}"
-          python3 patch_single_package.py --workingDir $working_dir --packageName ${{ parameters.packageName }} --azureLinuxBranch ${{ parameters.azureLinuxVersion }} --cve ${{ parameters.cveList }} --patchURL ${{ parameters.patchList }}
-        else
-          echo "Patching all packages"
+        if [ "${{ parameters.packageName }}" == "Not-Applicable" ]; then
+          echo "Patching all packages in Azure Linux ${{ parameters.azureLinuxVersion }}"
           python3 patch_all_package.py --workingDir $working_dir
+        else
+          echo "Patching package: ${{ parameters.packageName }} with CVEs: ${{ parameters.cveList }}"
+          python3 patch_single_package.py --workingDir $working_dir --packageName ${{ parameters.packageName }} --azureLinuxBranch ${{ parameters.azureLinuxVersion }} --cve ${{ parameters.cveList }} --patchURL ${{ parameters.patchList }}
         fi
+    env:
+      GITHUB_NAME: "$(GITHUB_NAME)"
+      GITHUB_USERNAME: "$(GITHUB_USERNAME)"
+      GITHUB_EMAIL: "$(GITHUB_EMAIL)"
+      GITHUB_TOKEN: "$(GITHUB_TOKEN)"
+      MY_BUILDID: "$(Build.BuildId)"
+    
+  - task: PublishPipelineArtifact@1
+    displayName: 'Publish Backport Candidates'
+    condition: eq('${{ parameters.attemptBackport }}', true)
+    inputs:
+      targetPath: '/tmp/backport_candidates/'
+      publishLocation: 'pipeline'
+      artifact: 'backport_candidates'
 
-        # if file summary.text exists, print it
-        if [ -f $working_dir/summary.txt ]; then
-          echo "Summary of patching:"
-          cat $working_dir/summary.txt
+  - task: AzureCLI@2
+    displayName: "Patch with backporting"
+    continueOnError: true
+    condition: eq('${{ parameters.attemptBackport }}', true)
+    inputs:
+      azureSubscription: autosec-astrolabe-interface
+      scriptType: bash
+      scriptLocation: inlineScript
+      inlineScript: |
+        set -ex
+        echo "Starting Autosec tool with backporting"
+        working_dir=$(pwd)/autosec
+        echo "Working directory: $working_dir"
+        
+        if [ "${{ parameters.packageName }}" == "Not-Applicable" ]; then
+          echo "Patching all packages in Azure Linux ${{ parameters.azureLinuxVersion }}"
+          python3 patch_all_package.py --workingDir $working_dir --tryBackport
         else
-          echo "No summary file found."
+          echo "Patching package: ${{ parameters.packageName }} with CVEs: ${{ parameters.cveList }}"
+          python3 patch_single_package.py --workingDir $working_dir --packageName ${{ parameters.packageName }} --azureLinuxBranch ${{ parameters.azureLinuxVersion }} --cve ${{ parameters.cveList }} --patchURL ${{ parameters.patchList }} --tryBackport
         fi
+    env:
+      GITHUB_NAME: "$(GITHUB_NAME)"
+      GITHUB_USERNAME: "$(GITHUB_USERNAME)"
+      GITHUB_EMAIL: "$(GITHUB_EMAIL)"
+      GITHUB_TOKEN: "$(GITHUB_TOKEN)"
+      MY_BUILDID: "$(Build.BuildId)"
 
-        mkdir /tmp/output
+  - task: AzureCLI@2
+    displayName: "PR Publishing"
+    continueOnError: true
+    inputs:
+      azureSubscription: autosec-astrolabe-interface
+      scriptType: bash
+      scriptLocation: inlineScript
+      inlineScript: |
+        set -ex
+        working_dir=$(pwd)/autosec
+        python3 publish_pr.py --workingDir $working_dir
+    env:
+      GITHUB_NAME: "$(GITHUB_NAME)"
+      GITHUB_USERNAME: "$(GITHUB_USERNAME)"
+      GITHUB_EMAIL: "$(GITHUB_EMAIL)"
+      GITHUB_TOKEN: "$(GITHUB_TOKEN)"
+      MY_BUILDID: "$(Build.BuildId)"
+
+  - bash: |
+      working_dir=$(pwd)/autosec
+      mkdir /tmp/logs
         echo "Current directory: $(pwd)"
         ls -lrt
         echo "Working directory: $working_dir"
@@ -99,18 +167,21 @@ jobs:
         if [ -z "$(ls $working_dir/*.csv 2>/dev/null)" ]; then
           echo "No CSV files found in the working directory."
         else
-          cp $working_dir/*.csv /tmp/output/
-          echo "CSV files copied to /tmp/output/"
+          cp $working_dir/*.csv /tmp/logs/
+          echo "CSV files copied to /tmp/logs/"
+          ls -lrt /tmp/logs/
         fi
-    env:
-      GITHUB_NAME: "$(GITHUB_NAME)"
-      GITHUB_USERNAME: "$(GITHUB_USERNAME)"
-      GITHUB_EMAIL: "$(GITHUB_EMAIL)"
-      GITHUB_TOKEN: "$(GITHUB_TOKEN)"
+    displayName: 'Copy logs for publishing'
 
   - task: PublishPipelineArtifact@1
     displayName: 'Publish Autosec Artifacts'
     inputs:
-      targetPath: '/tmp/output'
+      targetPath: '/tmp/logs'
       publishLocation: 'pipeline'
-      artifact: 'autosec_output'
+      artifact: 'autosec_logs'
+
+  - bash: |
+      echo "Printing summary of the patching process"
+      python3 summary.py
+      cat summary.txt
+    displayName: 'Summary'
diff --git a/db_util.py b/db_util.py
index 352944b..ca8df29 100644
--- a/db_util.py
+++ b/db_util.py
@@ -24,17 +24,34 @@ def get_sql_connection():
         f"Connection Timeout=30;"
     )
 
-    credential = AzureCliCredential()
-    token_bytes = credential.get_token(
-        "https://database.windows.net/.default"
-    ).token.encode("UTF-16-LE")
-    token_struct = struct.pack(f"<I{len(token_bytes)}s", len(token_bytes), token_bytes)
-
-    SQL_COPT_SS_ACCESS_TOKEN = 1256
-    conn = pyodbc.connect(
-        connection_string, attrs_before={SQL_COPT_SS_ACCESS_TOKEN: token_struct}
-    )
-    return conn
+    max_retries = 3
+    # retry with exponential backoff
+    for attempt in range(max_retries):
+        try:
+            credential = AzureCliCredential()
+            token_bytes = credential.get_token(
+                "https://database.windows.net/.default"
+            ).token.encode("UTF-16-LE")
+            token_struct = struct.pack(
+                f"<I{len(token_bytes)}s", len(token_bytes), token_bytes
+            )
+
+            SQL_COPT_SS_ACCESS_TOKEN = 1256
+            conn = pyodbc.connect(
+                connection_string, attrs_before={SQL_COPT_SS_ACCESS_TOKEN: token_struct}
+            )
+            return conn
+        except Exception as e:
+            print(f"Attempt {attempt + 1} failed: {e}")
+            if attempt < max_retries - 1:
+                wait_time = 2**attempt  # Exponential backoff
+                print(f"Connection failed, retrying in {wait_time} seconds...")
+                time.sleep(wait_time)
+            else:
+                print("Max retries reached. Could not connect to the database.")
+                raise e
+
+    return None
 
 
 """
diff --git a/patch_all_package.py b/patch_all_package.py
index 3d890c1..2f7f185 100644
--- a/patch_all_package.py
+++ b/patch_all_package.py
@@ -5,11 +5,12 @@ It calls the `patch_single_package.py` script to prepare patches for each CVE an
 
 import argparse
 import time
-import requests  # Add this import
+import requests
 
 import api_utils
 import db_util
 import patch_single_package
+from utils.file_util import FileUtil
 
 
 def convert_to_patch_url(url: str) -> str:
@@ -186,7 +187,7 @@ def sort_package_wise_cves(package_wise_cves: dict) -> dict:
     return sorted_package_wise_cves
 
 
-def patch_all_package(workingDir: str):
+def patch_all_package(workingDir: str, try_backport: bool):
     """
     Fetch all CVE data from Astrolabe and try to fix them.
     It calls the `patch_single_package.py` script to prepare patches for each CVE and publishes them.
@@ -218,11 +219,12 @@ def patch_all_package(workingDir: str):
                 filtered_mariner_ids.append(mariner_id)
             try:
                 patch_single_package.patch_single_package(
-                    packageName=package,
-                    cve=filtered_cves,
-                    azureLinuxBranch=branch,
-                    patchURL=filtered_patches,
-                    workingDir=workingDir,
+                    package_name=package,
+                    cve_list=filtered_cves,
+                    azure_linux_version=branch,
+                    patch_link_list=filtered_patches,
+                    working_dir=workingDir,
+                    try_backport=try_backport,
                 )  # not sending filtered_mariner_ids as they are not yet supported in patch_single_package
             except Exception as e:
                 print(
@@ -231,6 +233,32 @@ def patch_all_package(workingDir: str):
                 db_util.insert_failed_cve(cve, package, branch, patch_url, db_conn)
 
 
+def patch_all_package_with_backport(workingDir: str):
+    file_util = FileUtil()
+    package_version_dict = {}
+    for (
+        azure_linux_version,
+        package,
+        cve_id,
+        aux_dir,
+    ) in file_util.iter_directory_structure():
+        if azure_linux_version not in package_version_dict:
+            package_version_dict[azure_linux_version] = set()
+        package_version_dict[azure_linux_version].add(package)
+
+    for version, packages in package_version_dict.items():
+        for package in packages:
+            print(f"### Patching package {package} for branch {version} with backport")
+            patch_single_package.patch_single_package(
+                package_name=package,
+                cve_list=[],
+                azure_linux_version=version,
+                patch_link_list=[],
+                working_dir=workingDir,
+                try_backport=True,
+            )
+
+
 def getArgs():
     """
     Get the arguments from command line
@@ -243,9 +271,18 @@ def getArgs():
         help="Working directory to prepare patches",
         required=True,
     )
-    return parser.parse_args()
+
+    parser.add_argument(
+        "--tryBackport",
+        action="store_true",
+        help="Try to backport the patch if it is not available in the patch URL",
+    )
+
+    args = parser.parse_args()
+
+    return args.workingDir, args.tryBackport
 
 
 if __name__ == "__main__":
-    workingDir = getArgs().workingDir
-    patch_all_package(workingDir)
+    workingDir, try_backport = getArgs()
+    patch_all_package(workingDir, try_backport=try_backport)
diff --git a/patch_single_package.py b/patch_single_package.py
index a82ef41..0fabb99 100644
--- a/patch_single_package.py
+++ b/patch_single_package.py
@@ -1,8 +1,9 @@
 import os
 import argparse
-from prepare_patch import prepare_patches
-from publishing import publish_patch
+from prepare_patch import prepare_patches_combined
+from publish_pr import publish_patch
 from api_utils import get_ai_final_verdict
+from utils.file_util import FileUtil
 
 
 def getArgs():
@@ -47,6 +48,12 @@ def getArgs():
         required=True,
     )
 
+    parser.add_argument(
+        "--tryBackport",
+        action="store_true",
+        help="Try to backport the patch if it is not available in the patch URL",
+    )
+
     args = parser.parse_args()
     return (
         args.packageName,
@@ -54,15 +61,13 @@ def getArgs():
         args.azureLinuxBranch,
         args.patchURL,
         args.workingDir,
+        args.tryBackport,
     )
 
 
-def write_summary(
-    packageName, total_cves, fixed_cves, backport_fixed_cves, working_dir
-):
-    failed_cves = [
-        cve for cve in total_cves if cve not in fixed_cves + backport_fixed_cves
-    ]
+def write_summary(packageName, total_cves, fixed_cves, working_dir, try_backport):
+    # convert all this into a json file.
+    failed_cves = [cve for cve in total_cves if cve not in fixed_cves]
 
     summary_file = f"{working_dir}/summary.txt"
     # if summary file doesn't exist, create it
@@ -73,70 +78,111 @@ def write_summary(
     with open(summary_file, "a") as f:
         f.write(f"Package Name: {packageName}\n")
         f.write(f"Total CVEs: {len(total_cves)}\n")
-        f.write(f"Fixed CVEs: {len(fixed_cves)}\n")
-        f.write(f"Backport Fixed CVEs: {len(backport_fixed_cves)}\n")
+        if try_backport:
+            f.write(f"Backport Fixed CVEs: {len(fixed_cves)}\n")
+        else:
+            f.write(f"Fixed CVEs: {len(fixed_cves)}\n")
+
         f.write(f"Failed CVEs: {len(failed_cves)}\n")
         f.write("Failed CVEs Details:\n")
         for cve in failed_cves:
             f.write(f"- {cve}\n")
-        f.write("Fixed CVEs Details:\n")
+
+        if try_backport:
+            f.write("Backport Fixed CVEs Details:\n")
+        else:
+            f.write("Fixed CVEs Details:\n")
         for cve in fixed_cves:
             f.write(f"- {cve}\n")
-        f.write("Backport Fixed CVEs Details:\n")
-        for cve in backport_fixed_cves:
-            f.write(f"- {cve}\n")
         f.write("\n")
         f.write("\n\n\n")
 
 
-def patch_single_package(packageName, cve, azureLinuxBranch, patchURL, workingDir):
+def scan_aux_dir_for_backport_candidates(azure_linux_version, package_name):
     """
-    Prepare patches for a single package and publish them.
+    Scan the aux directory for backport candidates.
+    Returns a tuple of (cve_list, patch_link_list).
     """
-    print(f"Package Name: {packageName}")
-    print(f"CVE Value: {cve}")
-    print(f"Azurelinux Branch: {azureLinuxBranch}")
-    print(f"Patch URL: {patchURL}")
-    print(f"Working Directory: {workingDir}")
-
-    if len(patchURL) == 0:
-        patchURL = [get_ai_final_verdict(packageName, cveValue) for cveValue in cve]
-    if len(patchURL) != len(cve):
+    file_util = FileUtil()
+    cve_list = []
+    patch_link_list = []
+
+    for (
+        azure_linux_version,
+        package_name,
+        cve_id,
+        aux_dir,
+    ) in file_util.iter_directory_structure():
+        if azure_linux_version == azure_linux_version and package_name == package_name:
+            status = file_util.get_status(azure_linux_version, package_name, cve_id)
+            if status != "FAIL":
+                continue
+            cve_list.append(cve_id)
+            patch_link = file_util.get_aux_directory(
+                azure_linux_version, package_name, cve_id
+            )
+            patch_link_list.append(patch_link)
+
+    return cve_list, patch_link_list
+
+
+def patch_single_package(
+    package_name,
+    cve_list,
+    azure_linux_version,
+    patch_link_list,
+    working_dir,
+    try_backport,
+):
+    """
+    Prepare patches for a single package.
+
+    Then try_backport is true, the cve_list and patch_link_list is taken from aux_dir.
+    """
+    print(f"Package Name: {package_name}")
+    print(f"CVE Value: {cve_list}")
+    print(f"Azurelinux Branch: {azure_linux_version}")
+    print(f"Patch URL: {patch_link_list}")
+    print(f"Working Directory: {working_dir}")
+
+    if try_backport:
+        cve_list, patch_link_list = scan_aux_dir_for_backport_candidates(
+            azure_linux_version=azure_linux_version,
+            package_name=package_name,
+        )
+
+    if len(patch_link_list) == 0:
+        patch_link_list = [
+            get_ai_final_verdict(azure_linux_version, package_name, cveValue)
+            for cveValue in cve_list
+        ]
+    if len(patch_link_list) != len(cve_list):
         raise ValueError(
             "Number of patch URLs must match number of CVE values provided."
         )
 
-    all_success, fixed_cves, backport_fixed_cves = prepare_patches(
-        packageName=packageName,
-        cveValue=cve,
-        azureLinuxBranch=azureLinuxBranch,
-        patchURL=patchURL,
-        workingDir=workingDir,
-    )
-
-    publish_patch(
-        packageName=packageName,
-        cveValues=fixed_cves + backport_fixed_cves,
-        azureLinuxBranch=azureLinuxBranch,
-        workingDir=workingDir,
-    )
-
-    write_summary(
-        packageName=packageName,
-        total_cves=cve,
-        fixed_cves=fixed_cves,
-        backport_fixed_cves=backport_fixed_cves,
-        working_dir=workingDir,
+    if not try_backport:
+        file_util = FileUtil()
+        file_util.create_dir_structure(azure_linux_version, package_name, cve_list)
+
+    prepare_patches_combined(
+        package_name=package_name,
+        cve_list=cve_list,
+        azure_linux_version=azure_linux_version,
+        patch_link_list=patch_link_list,
+        working_dir=working_dir,
+        try_backport=try_backport,
     )
 
 
 if __name__ == "__main__":
-    packageName, cve, azureLinuxBranch, patchURL, workingDir = getArgs()
+    packageName, cve, azureLinuxBranch, patchURL, workingDir, try_backport = getArgs()
 
     patch_single_package(
-        packageName=packageName,
-        cve=cve,
-        azureLinuxBranch=azureLinuxBranch,
-        patchURL=patchURL,
-        workingDir=workingDir,
+        package_name=packageName,
+        cve_list=cve,
+        azure_linux_version=azureLinuxBranch,
+        patch_link_list=patchURL,
+        working_dir=workingDir,
+        try_backport=try_backport,
     )
diff --git a/prepare_patch.py b/prepare_patch.py
index 068b889..5ef4cba 100644
--- a/prepare_patch.py
+++ b/prepare_patch.py
@@ -10,40 +10,43 @@ Also, for failed patches, it saves the details in a CSV file named `failed_cves.
 
 import json
 import os
+import pprint
 import subprocess
 import time
-import pprint
 
 import pandas as pd
 import requests
 
-from api_utils import getJson
-from db_util import get_sql_connection, insert_failed_cve
 from ai_utils import CveDetails
+from api_utils import getJson, get_ai_final_verdict
+from db_util import get_sql_connection, insert_failed_cve
+from publish_pr import run_command
 from swe_agent import SweAgentCaller
-from publishing import git_command
+from utils.file_util import FileUtil
+
 
 GITHUB_USERNAME = os.environ.get("GITHUB_USERNAME")
 GITHUB_EMAIL = os.environ.get("GITHUB_EMAIL")
 GITHUB_TOKEN = os.environ.get("GITHUB_TOKEN")
 
 
-
 def cloneAzureLinuxIfDoesntExist(branch, working_dir):
     org = "azurelinux-security"
     repo_name = "azurelinux"
-    git_url = f"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@github.com/{org}/{repo_name}.git"
+    git_url = (
+        f"https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@github.com/{org}/{repo_name}.git"
+    )
     repo_path = working_dir + repo_name
 
     if not os.path.exists(repo_path):
-        git_command(f"git clone {git_url} {repo_path}")
+        run_command(f"git clone {git_url} {repo_path}")
     os.chdir(repo_path)
-    git_command("git clean -fdx")
-    git_command("git remote add upstream https://github.com/microsoft/azurelinux.git")
-    git_command(f"git fetch --all")
-    git_command(f"git checkout -B origin/{branch} upstream/{branch}")
-    git_command(f"git rebase upstream/{branch}")
-    git_command(f"git push origin HEAD:{branch} --force-with-lease")
+    run_command("git clean -fdx")
+    run_command("git remote add upstream https://github.com/microsoft/azurelinux.git")
+    run_command(f"git fetch --all")
+    run_command(f"git checkout -B origin/{branch} upstream/{branch}")
+    run_command(f"git rebase upstream/{branch}")
+    run_command(f"git push origin HEAD:{branch} --force-with-lease")
 
     return repo_path
 
@@ -54,7 +57,7 @@ def downloadTar(fileName, downloadDir):
 
     Args:
         fileName (str): tar file name
-        workingDir (str): working directory to download tars
+        working_dir (str): working directory to download tars
 
     Returns:
         None
@@ -94,14 +97,14 @@ def removeFileExtension(fileName):
     Returns:
         str: file name without extension
     """
-    extensions = [".tar.gz", "tar.xz", ".tar"]
+    extensions = [".tar.gz", ".tar.xz", ".tar"]
     for extension in extensions:
         if fileName.endswith(extension):
             return fileName[: -len(extension)]
     return fileName
 
 
-def prepareTarFiles(cveJson, targetVersion, workingDir, deleteExisting=False):
+def prepareTarFiles(cveJson, targetVersion, working_dir, deleteExisting=False):
     """
     download tar files and untar them into folder structure like:
         package_name -> cve_id -> tar file name -> tar content
@@ -111,14 +114,14 @@ def prepareTarFiles(cveJson, targetVersion, workingDir, deleteExisting=False):
     Args:
         cveJson (dict): json object containing cve details (from Astrolabe)
         targetVersion (str): Azurelinux's version to consider (2.0 or 3.0)
-        workingDir (str): working directory to download tars
+        working_dir (str): working directory to download tars
 
     Returns:
         None
     """
     shifted_tars = []
     for packageDir, tarFilePath, affectedFiles in iter_files(
-        cveJson, targetVersion, workingDir
+        cveJson, targetVersion, working_dir
     ):
         tarDir = removeFileExtension(tarFilePath)
 
@@ -140,10 +143,7 @@ def prepareTarFiles(cveJson, targetVersion, workingDir, deleteExisting=False):
             os.makedirs(tarDir)
 
         # untar the tar file
-        os.system(
-            f"tar -xf {packageDir}/{tarFileName} -C {tarDir} > tar_output.txt 2>&1"
-        )
-        os.system("head -n 10 tar_output.txt")
+        os.system(f"tar -xf {packageDir}/{tarFileName} -C {tarDir}")
 
         # if after untarging, it produces a folder with the same name as tar file,
         # then move the contents of that folder to the tarDir and delete the folder
@@ -182,14 +182,14 @@ def get_tarball_names_from_signature(package, working_dir):
     return res
 
 
-def getTarsOfPackageFromSpec(package, workingDir):
+def getTarsOfPackageFromSpec(package, working_dir):
     allTars = []
-    spec_loc = f"{workingDir}/azurelinux/SPECS/{package}/{package}.spec"
+    spec_loc = f"{working_dir}/azurelinux/SPECS/{package}/{package}.spec"
     result = subprocess.run(["spectool", spec_loc], capture_output=True, text=True)
     output = result.stdout.split("\n")
 
     if result.stdout == "":
-        return get_tarball_names_from_signature(package, workingDir)
+        return get_tarball_names_from_signature(package, working_dir)
 
     for line in output:
         if line.lower().startswith("source"):
@@ -202,7 +202,7 @@ def getTarsOfPackageFromSpec(package, workingDir):
     return allTars
 
 
-def iter_files(cveJson, targetVersion, workingDir):
+def iter_files(cveJson, targetVersion, working_dir):
     """
     Iterate over all entries in cveJson and yield the
         1. packageDir
@@ -212,7 +212,7 @@ def iter_files(cveJson, targetVersion, workingDir):
     Args:
         cveJson (dict): json object containing cve details (from Astrolabe)
         targetVersion (str): Azurelinux's version to consider (2.0 or 3.0)
-        workingDir (str): working directory to download tars
+        working_dir (str): working directory to download tars
 
     Returns:
         tuple: (packageDir, tarFilePath, affectedFiles)
@@ -228,9 +228,9 @@ def iter_files(cveJson, targetVersion, workingDir):
             continue
 
         if pkgName not in package_tar_map:
-            package_tar_map[pkgName] = getTarsOfPackageFromSpec(pkgName, workingDir)
+            package_tar_map[pkgName] = getTarsOfPackageFromSpec(pkgName, working_dir)
 
-        packageDir = workingDir + "packages/" + pkgName
+        packageDir = working_dir + "packages/" + pkgName
 
         for file in cveObj["affectedFiles"]:
             tarballName = file["tarballName"]
@@ -326,72 +326,86 @@ def decodeFileName(fileName):
 
 
 def downloadPatchFile(
-    patchURL, workingDir, CVEValue, patch_from_file, deleteExisting=False
+    azure_linux_version,
+    package_name,
+    patch_url,
+    working_dir,
+    cve_id,
+    patch_from_file,
+    deleteExisting=False,
 ):
     """
     download patch file from azurelinux's storage.
 
     Args:
-        patchURL (str): URL of the patch file
-        workingDir (str): working directory to download patch
-        CVEValue (str): CVE value to prepare patches for
+        patch_url (str): URL of the patch file
+        working_dir (str): working directory to download patch
+        cve_id (str): CVE value to prepare patches for
 
     Returns:
         None
     """
     if deleteExisting:
-        os.system(f"rm {workingDir}/*.patch")
+        os.system(f"rm {working_dir}/*.patch")
     if patch_from_file:
-        # If patch_from_file is True, we assume that patchURL is a local file path
-        if not os.path.exists(patchURL):
-            raise FileNotFoundError(f"Patch file path {patchURL} does not exist.")
-        with open(patchURL, "rb") as f:
+        # If patch_from_file is True, we assume that patch_url is a local file path
+        if not os.path.exists(patch_url):
+            raise FileNotFoundError(f"Patch file path {patch_url} does not exist.")
+        with open(patch_url, "rb") as f:
             content = f.read()
-        with open(workingDir + f"/{CVEValue}.patch", "wb") as f:
+        with open(working_dir + f"/{cve_id}.patch", "wb") as f:
             f.write(content)
     else:
-        response = requests.get(patchURL)
-        if response.status_code == 200:
-            with open(workingDir + f"/{CVEValue}.patch", "wb") as f:
-                f.write(response.content)
-        else:
-            raise Exception("Failed to download patch file: " + patchURL)
+        os.system(f"wget {patch_url} -O {working_dir}/{cve_id}.patch")
+        if not os.path.exists(working_dir + f"/{cve_id}.patch"):
+            raise FileNotFoundError(
+                f"Patch file {working_dir}/{cve_id}.patch didn't download successfully."
+            )
+        file_util = FileUtil()
+        aux_patch_dir = file_util.get_aux_directory(
+            azure_linux_version, package_name, cve_id
+        )
+        subprocess.run(
+            f"cp {working_dir}/{cve_id}.patch {aux_patch_dir}",
+            shell=True,
+            check=True,
+        )
 
-    file_diff = getFileDiffMap(workingDir + f"/{CVEValue}.patch")
+    file_diff = getFileDiffMap(working_dir + f"/{cve_id}.patch")
     for file_name, diff in file_diff.items():
         # encode filename like avahi-core/wide-area.c to unix filename
         file_name = encodeFileName(file_name)
-        with open(workingDir + f"/{file_name}.patch", "w") as f:
+        with open(working_dir + f"/{file_name}.patch", "w") as f:
             f.write(diff)
 
 
-def findPatchFile(relativePath, workingDir):
+def findPatchFile(relative_path, working_dir):
     """
     Fuzzy search for the patch file in the working directory.
 
     Args:
-        relativePath (str): relative path of the patch file to search for. like /vendor/avahi-core/wide-area.c.patch
-        workingDir (str): Working directory to search in.
+        relative_path (str): relative path of the patch file to search for. like /vendor/avahi-core/wide-area.c.patch
+        working_dir (str): Working directory to search in.
     Returns:
         str: Path to the patch file if found, else None.
     """
-    if os.path.exists(workingDir + "/" + relativePath):
-        return workingDir + "/" + relativePath
+    if os.path.exists(working_dir + "/" + relative_path):
+        return working_dir + "/" + relative_path
 
     res = ""
     fileTrie = ReversePathTrie()  # TODO: No need to create a new trie every time
-    for file in os.listdir(workingDir):
-        if file.endswith(".patch") and os.path.isfile(os.path.join(workingDir, file)):
+    for file in os.listdir(working_dir):
+        if file.endswith(".patch") and os.path.isfile(os.path.join(working_dir, file)):
             fileTrie.insert(file)
 
-    res = fileTrie.search(relativePath + ".patch")
+    res = fileTrie.search(relative_path + ".patch")
 
     if len(res) == 1:
-        return workingDir + "/" + res[0]
+        return working_dir + "/" + res[0]
     else:
         # TODO: improve error handling
         raise ValueError(
-            f"Zero or Multiple patch files found for {relativePath}."
+            f"Zero or Multiple patch files found for {relative_path}."
             f"Please specify the correct one."
             f"Found: {res}"
         )
@@ -480,52 +494,96 @@ class ReversePathTrie:
             return []
 
 
+def save_modified_tar_file(tar_dir, package, cve_id, azure_linux_version):
+    """
+    After applying patches(either exising or AI patch verdict etc).
+    Saves tar file into a aux folder.
+    this folder will have tar of tar_dir and patch file.
+    """
+    file_util = FileUtil()
+
+    backport_candidates_dir = file_util.get_aux_directory(
+        azure_linux_version, package, cve_id
+    )
+
+    if not os.path.exists(backport_candidates_dir):
+        os.makedirs(backport_candidates_dir)
+
+    # copy the tar file to the backport candidates directory
+    tar_file_name = tar_dir.split("/")[-1]
+
+    os.system(
+        f"rm -rf {tar_dir}/.git"
+    )  # remove git directory because swe-agent wants single directory in the tar file
+
+    # create a tar file in the aux_directory which contains the tar_dir.
+    # whenever created tar is untared it should create a directory with the tar_file_name
+    # inside which the tar_dir contents will be present.
+    aux_tar_file_path = f"{backport_candidates_dir}/{tar_file_name}.tar.xz"
+    os.system(
+        f"tar -cJf {aux_tar_file_path} -C {tar_dir} ../{tar_file_name}"
+    )  # -C option is used to change directory to tar_dir before creating tar file
+
+
 def prepare_patches_single(
-    packageName, cveValue, azureLinuxBranch, patchURL, workingDir, patch_from_file=False
+    package_name,
+    cve_id,
+    azure_linux_version,
+    patch_url,
+    working_dir,
+    trying_backport=False,
 ):
     """
     Prepare patches for the given package and CVE value.
     Args:
-        packageName (str): Name of the package
-        cveValue (str): CVE value to prepare patches for
-        azureLinuxBranch (str): Azurelinux version to consider (2.0 or 3.0)
-        patchURL (str): URL to publish the patch
+        package_name (str): Name of the package
+        cve_id (str): CVE value to prepare patches for
+        azure_linux_version (str): Azurelinux version to consider (2.0 or 3.0)
+        patch_url (str): URL to publish the patch
     Returns:
         None
     """
-    print(f"Preparing patches for {packageName} with CVE {cveValue} and URL {patchURL}")
-    branch = "fasttrack/3.0" if azureLinuxBranch == "3.0" else "fasttrack/2.0"
-
-    downloadPatchFile(patchURL, workingDir, cveValue, patch_from_file, True)
-    print(f"Downloaded patch file for {cveValue} to {workingDir}")
-    if workingDir[-1] != "/":  # ensure workingDir ends with /
-        workingDir += "/"
+    print(f"Preparing patches for {package_name} with CVE {cve_id} and URL {patch_url}")
+    branch = "fasttrack/3.0" if azure_linux_version == "3.0" else "fasttrack/2.0"
+
+    downloadPatchFile(
+        azure_linux_version,
+        package_name,
+        patch_url,
+        working_dir,
+        cve_id,
+        trying_backport,
+        True,
+    )
+    print(f"Downloaded patch file for {cve_id} to {working_dir}")
+    if working_dir[-1] != "/":  # ensure working_dir ends with /
+        working_dir += "/"
 
     # Fetch the CVE details from astrolabe
-    cveJson = getJson(packageName, cveValue)
+    cveJson = getJson(package_name, cve_id)
 
     # download tar files
-    shiftedTars = prepareTarFiles(cveJson, azureLinuxBranch, workingDir, True)
+    shiftedTars = prepareTarFiles(cveJson, azure_linux_version, working_dir, True)
 
-    # for all package_name folder in workingDir, go to cveValue folder followed by tar file directory.
+    # for all package_name folder in working_dir, go to cve_id folder followed by tar file directory.
     # If it's not already a git directory then do following:
     # git init, git add all, git commit "initial commit"
 
     for packageDir, tarFilePath, affectedFiles in iter_files(
-        cveJson, azureLinuxBranch, workingDir
+        cveJson, azure_linux_version, working_dir
     ):
-        packageName = packageDir.split("/")[-1]
+        package_name = packageDir.split("/")[-1]
         tarDir = removeFileExtension(tarFilePath)
         if not os.path.exists(tarDir + "/.git"):
             os.system("git init " + tarDir)
             os.system("git -C " + tarDir + " add --force .")
-            os.system("git -C " + tarDir + ' commit -m "initial commit"')
+            run_command("git -C " + tarDir + ' commit -m "initial commit"')
         latestGitLog = os.popen("git -C " + tarDir + " log -1").read()
         if not "Apply existing CVE patches" in latestGitLog:
             # apply patch from corresponding spec files
-            specFileDir = f"{workingDir}/azurelinux/SPECS/{packageName}"
+            specFileDir = f"{working_dir}/azurelinux/SPECS/{package_name}"
             orderedPatchFiles = []
-            with open(specFileDir + f"/{packageName}.spec") as f:
+            with open(specFileDir + f"/{package_name}.spec") as f:
                 for line in f:
                     if line.startswith("Patch") and "CVE" in line:
                         orderedPatchFiles.append(line.split()[1])
@@ -533,7 +591,10 @@ def prepare_patches_single(
                 currDir = os.getcwd()
                 # TODO: create a function doesPatchApplyInsideTar and use it to decide directory below
                 os.chdir(tarDir)
-                os.system(f"patch -t -p1 < {specFileDir}/{patchFilePath}")
+                # -t option is used below to avoid asking which file to patch or other error confirmations
+                os.system(
+                    f"patch -t -p1 < {specFileDir}/{patchFilePath}"
+                )  # TODO: somehow avoid creating orig files here.
                 print(f"Applied patch {patchFilePath} to {os.getcwd()}")
                 os.chdir(currDir)
 
@@ -548,7 +609,7 @@ def prepare_patches_single(
 
     previousTarFilePath = ""
     for packageDir, tarFilePath, affectedFiles in iter_files(
-        cveJson, azureLinuxBranch, workingDir
+        cveJson, azure_linux_version, working_dir
     ):
         tarDir = removeFileExtension(tarFilePath)
         for affectedFile in affectedFiles:
@@ -557,7 +618,7 @@ def prepare_patches_single(
             patchFileName = encodeFileName(affectedFile)
             if tarNameOnly in shiftedTars:
                 patchFileName = encodeFileName("/".join(affectedFile.split("/")[2:]))
-            patchFilePath = findPatchFile(patchFileName, workingDir)
+            patchFilePath = findPatchFile(patchFileName, working_dir)
             if os.path.exists(patchFilePath):
                 currDir = os.getcwd()
                 # chdir to affectedFile's parent directory
@@ -588,48 +649,67 @@ def prepare_patches_single(
                     os.system(
                         f"patch --no-backup-if-mismatch {findOutput} < {patchFilePath}"
                     )
+                    file_util = FileUtil()
+                    file_util.set_status(
+                        azure_linux_version, package_name, cve_id, "PASS"
+                    )
+                    fixed_by = "BACKPORT" if trying_backport else "AUTOSEC"
+                    file_util.set_fixed_by(
+                        azure_linux_version, package_name, cve_id, fixed_by
+                    )
+                save_modified_tar_file(
+                    tarDir, package_name, cve_id, azure_linux_version
+                )
                 os.chdir(currDir)
 
     # Commit the changes with message "Fix CVE <cve_id> in <package_name>" in the tar file directories
     for packageDir, tarFilePath, affectedFiles in iter_files(
-        cveJson, azureLinuxBranch, workingDir
+        cveJson, azure_linux_version, working_dir
     ):
         tarDir = removeFileExtension(tarFilePath)
         os.system(f"git -C {tarDir} add .")
+        real_patch_url = patch_url
+        if trying_backport:
+            real_patch_url = get_ai_final_verdict(
+                azure_linux_version, package_name, cve_id
+            )
+        patch_commit_header = "Upstream Patch Reference: {real_patch_url}"
+        if trying_backport:
+            patch_commit_header = "[Backported] " + patch_commit_header
         os.system(
-            f'git -C {tarDir} commit -m "Fix CVE {cveValue} in {packageDir.split("/")[-1]}" -m "Upstream Patch Reference: {patchURL}"'
+            f'git -C {tarDir} commit -m "Fix CVE {cve_id} in {packageDir.split("/")[-1]}" -m {patch_commit_header}'
         )
 
     # For each tar file directory, do git format-patch HEAD~1 to generate patch files and store them in the SPECS directory.
     for packageDir, tarFilePath, affectedFiles in iter_files(
-        cveJson, azureLinuxBranch, workingDir
+        cveJson, azure_linux_version, working_dir
     ):
         tarDir = removeFileExtension(tarFilePath)
-        packageName = packageDir.split("/")[-1]
+        package_name = packageDir.split("/")[-1]
         os.system(f"git -C {tarDir} format-patch HEAD~1")
         os.system(
-            f"mv {tarDir}/000*.patch {workingDir}/azurelinux/SPECS/{packageName}/{cveValue}.patch"
+            f"mv {tarDir}/000*.patch {working_dir}/azurelinux/SPECS/{package_name}/{cve_id}.patch"
         )
 
     return unpatchedFiles
 
 
 def save_failed_cves(
-    cveValue, packageName, azureLinuxBranch, patchURL, workingDir, unpatchedFiles
+    cve_id, package_name, azureLinuxBranch, patch_url, working_dir, unpatchedFiles
 ):
     """
     Save the failed CVE details to a csv file.
     Args:
-        cveValue (str): CVE value
-        packageName (str): Name of the package
+        cve_id (str): CVE value
+        package_name (str): Name of the package
         azureLinuxBranch (str): Azurelinux version to consider (2.0 or 3.0)
-        patchURL (str): URL to publish the patch
-        workingDir (str): Working directory to save the file
+        patch_url (str): URL to publish the patch
+        working_dir (str): Working directory to save the file
         file_name (str): Name of the file to save the failed CVE details
         reason (str): Reason for failure
     """
     if len(unpatchedFiles) == 0:
-        print(f"No unpatched files for {cveValue} in {packageName}. Skipping saving.")
+        print(f"No unpatched files for {cve_id} in {package_name}. Skipping saving.")
         return
 
     csv_file_name = "failed_cves.csv"
@@ -639,15 +719,15 @@ def save_failed_cves(
     for file_name, reason in unpatchedFiles.items():
         failed_cve_data.append(
             {
-                "cveValue": cveValue,
-                "packageName": packageName,
+                "cve_id": cve_id,
+                "package_name": package_name,
                 "azureLinuxBranch": azureLinuxBranch,
-                "patchLinkIfAvailable": patchURL,
+                "patchLinkIfAvailable": patch_url,
                 "fileName": file_name,
                 "reason": reason,
             }
         )
-    file_path = os.path.join(workingDir, csv_file_name)
+    file_path = os.path.join(working_dir, csv_file_name)
 
     if os.path.exists(file_path):
         df = pd.read_csv(file_path)
@@ -659,29 +739,48 @@ def save_failed_cves(
     df.to_csv(file_path, index=False)
 
 
-def try_swe_agent_backport(packageName, azureLinuxBranch, workingDir, failed_cves):
+def get_backported_patches(
+    azure_linux_version: str,
+    package_name: str,
+    working_dir: str,
+    candidate_cves: list,
+    patch_link_list: list,
+) -> tuple:
+    """
+    Attempt to backport patches for the given package and CVEs.
+
+    Returns a tuple of two lists:
+    - List of CVEs that were successfully backported
+    - List of file paths to the backported patches
+    """
     fixed_cves = []
-    all_success = True
-    if len(failed_cves) == 0:
+    fixed_patch_paths = []
+    if len(candidate_cves) == 0:
         print("No failed CVEs to backport. Exiting.")
-        return all_success, fixed_cves
+        return fixed_cves, fixed_patch_paths
 
-    failed_cve_details = []
-    for cve_url in failed_cves:
-        cve, url = cve_url
+    candidate_cve_details = []
+    for cve, url in zip(candidate_cves, patch_link_list):
         cve_details = CveDetails(
+            azure_linux_version=azure_linux_version,
             cve_id=cve,
-            package_name=packageName,
+            package_name=package_name,
             patch_link=url,
-            working_dir=workingDir,
+            working_dir=working_dir,
         )
-        failed_cve_details.append(cve_details)
+        candidate_cve_details.append(cve_details)
     swe_agent = SweAgentCaller()
-    package_patch = swe_agent.get_patches(failed_cve_details)
+    package_patch = swe_agent.get_patches(candidate_cve_details)
 
     print("Patches received from swe-agent backport:")
     pprint.pprint(package_patch)
 
+    if package_patch is None or package_patch.get(package_name, None) is None:
+        print(
+            f"No backported patches found for {package_name} in the response from swe-agent."
+        )
+        return fixed_cves, fixed_patch_paths
+
     for package_name, val in package_patch.items():
         cve_id, patch_path = val
         output = subprocess.run(f"cat {patch_path}", shell=True, capture_output=True)
@@ -689,102 +788,80 @@ def try_swe_agent_backport(packageName, azureLinuxBranch, workingDir, failed_cve
             print(
                 f"Failed to read patch file {patch_path} for {package_name} with CVE {cve_id}. Error: {output.stderr.decode()}"
             )
-            all_success = False
             continue
         else:
-            print(f"Patch file content for {package_name} with CVE {cve_id}:")
+            print(
+                f"Backported Patch file content for {package_name} with CVE {cve_id}:"
+            )
             print(output.stdout)
-        failed_files = prepare_patches_single(
-            package_name,
-            cve_id,
-            azureLinuxBranch,
-            patch_path,
-            workingDir,
-            patch_from_file=True,
-        )
-
-        if len(failed_files) > 0:
-            all_success = False
-        else:
-            fixed_cves.append(cve_id)
+        fixed_cves.append(cve_id)
+        fixed_patch_paths.append(patch_path)
 
-    return all_success, fixed_cves
+    return fixed_cves, fixed_patch_paths
 
 
-def prepare_patches(
-    packageName,
-    cveValue,
-    azureLinuxBranch,
-    patchURL,
-    workingDir,
-    trying_backported_patches=False,
-):
+def prepare_patches_combined(
+    package_name: str,
+    cve_list: list,
+    azure_linux_version: str,
+    patch_link_list: list,
+    working_dir: str,
+    try_backport=False,
+) -> tuple:
     """
-    Prepare patches for the given package and CVE value.
-    Args:
-        packageName (str): Name of the package
-        cveValue (list): CVE value(s) to prepare patches for
-        azureLinuxBranch (str): Azurelinux version to consider (2.0 or 3.0)
-        patchURL (list): URL(s) to publish the patch
-    Returns:
-        None
+    Prepare patches for multiple CVEs in a single package.
+
+    Returns list of CVEs that were successfully fixed.
     """
-    if workingDir[-1] != "/":  # ensure workingDir ends with /
-        workingDir += "/"
+    if working_dir[-1] != "/":  # ensure working_dir ends with /
+        working_dir += "/"
+
+    fixed_cves = []
+    if try_backport:
+        print(f"Trying to backport patches for {package_name} with CVEs {cve_list}")
+
+        cve_list, patch_link_list = get_backported_patches(
+            azure_linux_version, package_name, working_dir, cve_list, patch_link_list
+        )
+
+    if len(cve_list) == 0:
+        print(f"No CVEs to prepare patches for {package_name}. Skipping.")
+        return fixed_cves
 
-    # Git clone azurelinux branch according to input branch
-    branch = "fasttrack/3.0" if azureLinuxBranch == "3.0" else "fasttrack/2.0"
-    cloneAzureLinuxIfDoesntExist(branch, workingDir)
+    # Git clone azurelinux branch according to input version
+    branch = "fasttrack/3.0" if azure_linux_version == "3.0" else "fasttrack/2.0"
+    cloneAzureLinuxIfDoesntExist(branch, working_dir)
     db_conn = get_sql_connection()
 
-    all_success = True
-    fixed_cves = []
-    backport_fixed_cves = []
-    failed_cves = []
-    total_cves = len(cveValue)
+    total_cves = len(cve_list)
     for i in range(total_cves):
-        cve = cveValue[i]
-        url = patchURL[i]
-        print(f"Preparing patches for {packageName} with CVE {cve} and URL {url}")
+        cve = cve_list[i]
+        url = patch_link_list[i]
+        print(f"Preparing patches for {package_name} with CVE {cve} and URL {url}")
         failed_files = prepare_patches_single(
-            packageName, cve, azureLinuxBranch, url, workingDir
+            package_name=package_name,
+            cve_id=cve,
+            azure_linux_version=azure_linux_version,
+            patch_url=url,
+            working_dir=working_dir,
+            trying_backport=try_backport,
         )
         save_failed_cves(
-            cve, packageName, azureLinuxBranch, url, workingDir, failed_files
+            cve, package_name, azure_linux_version, url, working_dir, failed_files
         )
         if len(failed_files) > 0:
-            failed_cves.append([cve, url])
-
-            # insert failed cve into database
+            print(
+                f"Failed to prepare patches for {cve} in {package_name}. "
+                f"Failed files: {failed_files}. "
+                f"Please check the logs for more details."
+            )
             insert_failed_cve(
                 cve_value=cve,
-                package_name=packageName,
-                azure_linux_branch=azureLinuxBranch,
+                package_name=package_name,
+                azure_linux_branch=azure_linux_version,
                 patch_link=url,
                 db_conn=db_conn,
             )
-            all_success = False
-            print(
-                f"Failed to prepare patches for {cve} in {packageName}. "
-                f"Failed files: {failed_files}. "
-                f"Please check the logs for more details."
-            )
         else:
             fixed_cves.append(cve)
-            print(f"Successfully prepared patches for {cve} in {packageName}")
-
-    if not all_success and not trying_backported_patches:
-        print(
-            f"Failed to prepare patches for {len(failed_cves)} out of {total_cves} CVEs in {packageName}. "
-            f"Trying to backport patch file from swe-agent."
-        )
-        all_success, backport_fixed_cves = try_swe_agent_backport(
-            packageName, azureLinuxBranch, workingDir, failed_cves
-        )
-        if len(backport_fixed_cves) == len(failed_cves):
-            print(
-                f"All {len(backport_fixed_cves)} CVEs in {packageName} were successfully fixed by swe-agent."
-            )
-            all_success = True
-
-    return all_success, fixed_cves, backport_fixed_cves
+            print(f"Successfully prepared patches for {cve} in {package_name}")
diff --git a/publishing.py b/publish_pr.py
similarity index 57%
rename from publishing.py
rename to publish_pr.py
index 5cec607..ede22b1 100644
--- a/publishing.py
+++ b/publish_pr.py
@@ -1,8 +1,11 @@
+import argparse
 import os
 import re
 import subprocess
 from typing import List
 
+from utils.file_util import FileUtil
+
 # Variables (similar to reference script)
 cve_data = {
     "cveValue": ["CVE-2024-26759", "CVE-2024-99999"],
@@ -20,6 +23,7 @@ GITHUB_TOKEN = os.environ.get("GITHUB_TOKEN")
 org = "azurelinux-security"
 repo_name = "azurelinux"
 
+
 def read_spec_file():
     with open(SPEC_FILE, "r") as file:
         return file.readlines()
@@ -48,34 +52,41 @@ def add_patch_entries(lines, cve_list):
     patch_indices = [
         i for i, line in enumerate(lines) if line.strip().startswith("Patch")
     ]
-    
+
     # Find all Source entries
     source_indices = [
         i for i, line in enumerate(lines) if line.strip().startswith("Source")
     ]
-    
+
     # Determine the last patch number
     last_patch_num = (
-        int(re.search(r"(\d+)", lines[patch_indices[-1]]).group(1)) if patch_indices else -1
+        int(re.search(r"(\d+)", lines[patch_indices[-1]]).group(1))
+        if patch_indices
+        else -1
     )
-    
+
     # If patches exist, add after the last patch
     # If no patches exist, add after the last source
-    insert_index = (patch_indices[-1] + 1) if patch_indices else (
-        source_indices[-1] + 1 if source_indices else 0
+    insert_index = (
+        (patch_indices[-1] + 1)
+        if patch_indices
+        else (source_indices[-1] + 1 if source_indices else 0)
     )
-    
+
     # Add new patches
     for idx, cve in enumerate(cve_list):
         patch_file_name = f"{cve}.patch"
-        patch_line = get_spaced_patch_declaration(last_patch_num + idx + 1, patch_file_name, lines)
+        patch_line = get_spaced_patch_declaration(
+            last_patch_num + idx + 1, patch_file_name, lines
+        )
         lines.insert(insert_index + idx, patch_line)
 
     return lines, last_patch_num + 1
 
+
 def bump_release(workingDir):
     changelog_entry = f"Patch for {', '.join(cve_data['cveValue'])}"
-    git_command(
+    run_command(
         f'{workingDir}/azurelinux/toolkit/scripts/update_spec.sh "{changelog_entry}" {SPEC_FILE}'
     )
 
@@ -90,30 +101,37 @@ def update_prep_section(lines, start_patch_num, num_new_patches):
 
     # Find setup or autosetup line after %prep
     setup_index = next(
-        (i for i in range(prep_index, len(lines)) 
-         if re.search(r"%setup|%autosetup", lines[i].strip())), None
+        (
+            i
+            for i in range(prep_index, len(lines))
+            if re.search(r"%setup|%autosetup", lines[i].strip())
+        ),
+        None,
     )
-    
+
     if setup_index is None:
         print("No %setup or %autosetup found in %prep section")
         return lines
 
     # Check for autosetup without -N flag
     has_autosetup_without_N = any(
-        re.search(r"%autosetup\s+-p1(?!\s+-N)|\s+%autopatch\s+-p1(?!\s+-N)", lines[i]) 
+        re.search(r"%autosetup\s+-p1(?!\s+-N)|\s+%autopatch\s+-p1(?!\s+-N)", lines[i])
         for i in range(prep_index, len(lines))
     )
-    
+
     if has_autosetup_without_N:
-        print("Found %autosetup -p1 or %autopatch -p1 without -N flag, skipping patch entries")
+        print(
+            "Found %autosetup -p1 or %autopatch -p1 without -N flag, skipping patch entries"
+        )
         return lines
 
     # Find the last patch entry or use setup_index if no patches exist
     patch_indices = [
-        i for i in range(setup_index, len(lines)) 
+        i
+        for i in range(setup_index, len(lines))
         if re.search(r"%patch\d+ -p1", lines[i])
     ]
-    
+
     insert_index = max(patch_indices) + 1 if patch_indices else setup_index + 1
 
     # Insert new patches after existing patches or after setup
@@ -122,7 +140,8 @@ def update_prep_section(lines, start_patch_num, num_new_patches):
 
     return lines
 
-def git_command(cmd):
+
+def run_command(cmd):
     result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
     if result.returncode != 0:
         print(f"Error: {result.stderr.strip()}")
@@ -141,29 +160,43 @@ def apply_changes(workingDir):
     write_spec_file(lines)
     bump_release(workingDir)
     if cve_data["azureLinuxBranch"] == "3.0":
-        git_command(
+        run_command(
             f"python3 {workingDir}/azurelinux/toolkit/scripts/update_toolchain_manifest.py --manifest_dir {workingDir}/azurelinux/toolkit/resources/manifests/package/ --specs {SPEC_FILE}"
-        ) # Update the toolchain manifest in 3.0 TODO 2.0
-    git_command(f"git checkout -b {BRANCH_NAME}")
+        )  # Update the toolchain manifest in 3.0 TODO 2.0
+    run_command(f"git checkout -b {BRANCH_NAME}")
     print(f"{BRANCH_NAME} created successfully.")
-    git_command("git add -A")
+    run_command("git add -A")
     print("Changes added successfully.")
-    git_command(
+    run_command(
         f'git commit -m "Patch {cve_data["packageName"]} for {", ".join(cve_data["cveValue"])}"'
     )
     print("Changes committed successfully.")
-    git_command(f"git push -u https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{repo_name}.git {BRANCH_NAME}")
+    run_command(
+        f"git push -u https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{repo_name}.git {BRANCH_NAME}"
+    )
     print("Changes pushed successfully.")
 
     # Temporarily remove GITHUB_TOKEN from env
     os.environ.pop("GITHUB_TOKEN", None)
-    git_command(f'echo "{GITHUB_TOKEN}" | gh auth login --hostname github.com --git-protocol https --with-token')
+    run_command(
+        f'echo "{GITHUB_TOKEN}" | gh auth login --hostname github.com --git-protocol https --with-token'
+    )
+
     # Create a pull request using GitHub CLI
-    basebranch = "fasttrack/2.0" if cve_data["azureLinuxBranch"] == "2.0" else "fasttrack/3.0"
+    basebranch = (
+        "fasttrack/2.0" if cve_data["azureLinuxBranch"] == "2.0" else "fasttrack/3.0"
+    )
+
+    pipeline_build_id = os.environ.get("MY_BUILDID", "unknown")
+    body = (
+        f'Auto Patch {cve_data["packageName"]} for {", ".join(cve_data["cveValue"])}.'
+        f"\n\n"
+        f"Autosec pipeline run -> https://dev.azure.com/mariner-org/mariner-chatbot/_build/results?buildId={pipeline_build_id}&view=results"
+    )
     create_pr = (
-        f'gh pr create --base {basebranch} --head {GITHUB_USERNAME}:{BRANCH_NAME} --repo microsoft/azurelinux '
+        f"gh pr create --base {basebranch} --head {GITHUB_USERNAME}:{BRANCH_NAME} --repo microsoft/azurelinux "
         f'--title "Patch {cve_data["packageName"]} for {", ".join(cve_data["cveValue"])}" '
-        f'--body "Auto Patch {cve_data["packageName"]} for {", ".join(cve_data["cveValue"])}; These patches apply cleanly like upstream"'
+        f'--body "{body}"'
     )
     os.system(create_pr)
     print("Pull request created successfully.")
@@ -178,12 +211,66 @@ def publish_patch(cveValues, packageName, azureLinuxBranch, workingDir):
 
     SPEC_FILE = f"{workingDir}/azurelinux/SPECS/{packageName}/{packageName}.spec"
     PATCH_FILE_NAME = f"{cveValues[0]}.patch"  # First one, for reference
-    username = git_command("git config user.name").split()[0].lower()
-    BRANCH_NAME = f"{username}-autosec/{cveValues[0]}/{packageName}/{azureLinuxBranch}"
+    username = run_command("git config user.name").split()[0].lower()
+    build_id = os.environ.get("MY_BUILDID", "unknown")  # Current pipeline build ID
+    BRANCH_NAME = f"{username}-autosec/{packageName}/{azureLinuxBranch}/{build_id}"
 
     apply_changes(workingDir)
 
 
+def getArgs():
+    """
+    Get the working_dir argument from the command line.
+    """
+    parser = argparse.ArgumentParser(description="Publish a patch for a package.")
+    parser.add_argument(
+        "--workingDir", required=True, help="Working directory for the operation."
+    )
+
+    args = parser.parse_args()
+    return args.workingDir
+
+
 if __name__ == "__main__":
+    workingDir = getArgs()
+    input_cves = {}  # version -> package -> cve list
+
+    file_util = FileUtil()
+    # collect all CVEs which were marked as PASS in the aux directory
+    for (
+        azure_linux_version,
+        package_name,
+        cve_id,
+        aux_dir,
+    ) in file_util.iter_directory_structure():
+        status = file_util.get_status(azure_linux_version, package_name, cve_id)
+        if status != "PASS":
+            print(
+                f"Skipping {cve_id} for {package_name} in {azure_linux_version} as status is {status}"
+            )
+            continue
+
+        if azure_linux_version not in input_cves:
+            input_cves[azure_linux_version] = {}
+        if package_name not in input_cves[azure_linux_version]:
+            input_cves[azure_linux_version][package_name] = []
+
+        input_cves[azure_linux_version][package_name].append(cve_id)
 
-    apply_changes()
\ No newline at end of file
+    if len(input_cves) == 0:
+        print("No CVEs found to publish patches for.")
+        exit(0)
+    # Iterate through the collected CVEs and publish patches
+    for azure_linux_version, packages in input_cves.items():
+        for package_name, cve_list in packages.items():
+            if not cve_list:
+                continue
+            print(
+                f"Publishing patches for {package_name} in {azure_linux_version} with CVEs: {', '.join(cve_list)}"
+            )
+            publish_patch(
+                cveValues=cve_list,
+                packageName=package_name,
+                azureLinuxBranch=azure_linux_version,
+                workingDir=workingDir,
+            )
diff --git a/summary.py b/summary.py
new file mode 100644
index 0000000..241faa8
--- /dev/null
+++ b/summary.py
@@ -0,0 +1,76 @@
+import os
+
+from utils.file_util import FileUtil
+
+
+def print_summary():
+    """
+    Print the summary of the patching process.
+    """
+    file_util = FileUtil()
+    status_detail = (
+        {}
+    )  # version: {"package_name": {"autosec": {"fixed": [cve1, cve2]}, "not_fixed": [cve3, cve4]}, "backport": []}}
+
+    for (
+        azure_linux_version,
+        package_name,
+        cve_id,
+        path_to_cve_dir,
+    ) in file_util.iter_directory_structure():
+        status = file_util.get_status(azure_linux_version, package_name, cve_id)
+        fixed_by = file_util.get_fixed_by(azure_linux_version, package_name, cve_id)
+
+        if azure_linux_version not in status_detail:
+            status_detail[azure_linux_version] = {}
+        if package_name not in status_detail[azure_linux_version]:
+            status_detail[azure_linux_version][package_name] = {}
+        if fixed_by not in status_detail[azure_linux_version][package_name]:
+            status_detail[azure_linux_version][package_name][fixed_by] = {}
+        if status not in status_detail[azure_linux_version][package_name][fixed_by]:
+            status_detail[azure_linux_version][package_name][fixed_by][status] = []
+        status_detail[azure_linux_version][package_name][fixed_by][status].append(
+            cve_id
+        )
+
+    """
+    Summary for Azure Linux Version 2.0
+    Package Name: python
+    Total CVEs: 7
+        Fixed By: autosec (Total 7 cves)
+        PASS CVEs: CVE-2024-66666, CVE-2024-44444, CVE-2024-22222
+        FAIL CVEs: CVE-2024-55555, CVE-2024-33333, CVE-2024-11111, CVE-2024-11187
+    Package Name: bind
+    Total CVEs: 7
+        Fixed By: backport (Total 2 cves)
+        PASS CVEs: CVE-2024-66666
+        FAIL CVEs: CVE-2024-33333
+        Fixed By: autosec (Total 5 cves)
+        PASS CVEs: CVE-2024-44444, CVE-2024-22222
+        FAIL CVEs: CVE-2024-55555, CVE-2024-11111, CVE-2024-11187
+    """
+    summary_file = "summary.txt"
+    os.system(f"touch {summary_file}")
+    with open(summary_file, "a") as f:
+        for azure_linux_version, packages in status_detail.items():
+            f.write(f"Summary for Azure Linux Version {azure_linux_version}\n")
+            for package_name, fixed_by_status in packages.items():
+                f.write(f"  Package Name: {package_name}\n")
+                total_cves = sum(
+                    len(cves)
+                    for fixed_by, status_cves in fixed_by_status.items()
+                    for status, cves in status_cves.items()
+                )
+                f.write(f"  Total CVEs: {total_cves}\n")
+                for fixed_by, status_cves in fixed_by_status.items():
+                    cves_count = sum(len(cves) for status, cves in status_cves.items())
+                    f.write(f"    Fixed By: {fixed_by} (Total {cves_count} cves)\n")
+                    for status, cve_list in status_cves.items():
+                        f.write(f"      {status} CVEs: {', '.join(cve_list)}\n")
+            f.write("\n")
+        f.write("\n\n\n")
+
+
+if __name__ == "__main__":
+    print_summary()
+    print("Summary printed to summary.txt")
diff --git a/swe_agent.py b/swe_agent.py
index a0448d1..44e261a 100644
--- a/swe_agent.py
+++ b/swe_agent.py
@@ -1,5 +1,6 @@
 import logging
 import os
+import subprocess
 import time
 import zipfile
 
@@ -9,6 +10,7 @@ from threading import Semaphore, Thread, Lock
 from azure.identity import AzureCliCredential
 
 from ai_utils import CveDetails
+from utils.file_util import FileUtil
 
 logger = logging.getLogger(__name__)
 
@@ -20,7 +22,7 @@ class SweAgentCaller:
     This class returns the patch created by the SWE agent pipeline or None if no patch is created.
     """
 
-    max_pipeline_wait_minutes = 40  # Maximum wait time in minutes
+    max_pipeline_wait_minutes = 70  # Maximum wait time in minutes
 
     def get_ado_auth_token(self):
         """Obtains the Azure DevOps authentication token using Azure CLI credentials."""
@@ -76,25 +78,27 @@ class SweAgentCaller:
 
     def __trigger_swe_pipeline(
         self,
-        package_git_repo: str,
-        package_base_commit: str,
-        cve_patch_commit: str,
+        azure_linux_version: str,
         package_name: str,
         cve_id: str,
     ):
         PIPELINE_ID = 5038  # Replace with actual pipeline ID
         pipeline_branch = "bala/single-agent-for-any-git"
+        build_id = os.environ.get("MY_BUILDID", "12345678")  # Default for testing
+        file_util = FileUtil()
+        suffix_path = file_util.get_suffix_path(
+            azure_linux_version, package_name, cve_id
+        )
         payload = {
             "stagesToSkip": [],
             "resources": {
                 "repositories": {"self": {"refName": f"refs/heads/{pipeline_branch}"}}
             },
             "templateParameters": {
-                "git_repo": package_git_repo,
-                "base_commit": package_base_commit,
-                "ref_commit": cve_patch_commit,
-                "package_name": package_name,
                 "cve_id": cve_id,
+                "parent_build_id": build_id,
+                "artifact_folder_name": "backport_candidates",
+                "suffix_path": suffix_path,
             },
             "variables": {},
         }
@@ -122,7 +126,14 @@ class SweAgentCaller:
                 logger.info(f"Error checking pipeline status: {e}")
                 time.sleep(30)
 
-    def __extract_patch_from_zip(self, zip_url: str, cve_id: str, working_dir: str):
+    def __extract_patch_from_zip(
+        self,
+        zip_url: str,
+        azure_linux_version: str,
+        package_name: str,
+        cve_id: str,
+        working_dir: str,
+    ):
         """
         Extracts the patch from the zip file and returns the patch file path.
         """
@@ -148,10 +159,24 @@ class SweAgentCaller:
                 f"Patch file {patch_file_path} not found in extracted files."
             )
 
+        file_util = FileUtil()
+        aux_path = file_util.get_aux_directory(
+            azure_linux_version, package_name, cve_id
+        )
+        aux_path = os.path.join(aux_path, f"backported_{cve_id}.patch")
+        # Copy the patch file to the auxiliary directory
+        subprocess.run(
+            ["cp", patch_file_path, aux_path], check=True, capture_output=True
+        )
         return patch_file_path
 
     def __download_and_extract_patch(
-        self, build_id: str, cve_id: str, working_dir: str
+        self,
+        build_id: str,
+        azure_linux_version: str,
+        package_name: str,
+        cve_id: str,
+        working_dir: str,
     ):
         """
         Downloads the patch from the SWE agent pipeline and returns the patch file path.
@@ -168,7 +193,9 @@ class SweAgentCaller:
             if artifact["name"] == "backported":
                 zip_url = artifact["resource"]["downloadUrl"]
                 logger.info(f"Downloading patch from: {zip_url}")
-                file_path = self.__extract_patch_from_zip(zip_url, cve_id, working_dir)
+                file_path = self.__extract_patch_from_zip(
+                    zip_url, azure_linux_version, package_name, cve_id, working_dir
+                )
                 logger.info(f"Patch extracted to: {file_path}")
                 return file_path
 
@@ -178,23 +205,26 @@ class SweAgentCaller:
         Returns the patch file path.
         """
         try:
-            package_git_repo = cve_detail.get_git_repo()
-            package_base_commit = cve_detail.get_package_commit()
-            cve_patch_commit = cve_detail.get_patch_commit()
+            # package_git_repo = cve_detail.get_git_repo()
+            # package_base_commit = cve_detail.get_package_commit()
+            # cve_patch_commit = cve_detail.get_patch_commit()
             package_name = cve_detail.package_name
             cve_id = cve_detail.cve_id
+            azure_linux_version = cve_detail.azure_linux_version
             print(f"__get_patches_worker: Processing {package_name} for CVE {cve_id}")
             response = self.__trigger_swe_pipeline(
-                package_git_repo,
-                package_base_commit,
-                cve_patch_commit,
+                azure_linux_version,
                 package_name,
                 cve_id,
             )
-            build_id = response["id"]
-            self.__wait_for_pipeline_completion(build_id)
+            swe_agent_build_id = response["id"]
+            self.__wait_for_pipeline_completion(swe_agent_build_id)
             patch_file_path = self.__download_and_extract_patch(
-                build_id, cve_id, cve_detail.working_dir
+                swe_agent_build_id,
+                azure_linux_version,
+                package_name,
+                cve_id,
+                cve_detail.working_dir,
             )
             cve_detail.patch_link = patch_file_path
             logger.info(
diff --git a/utils/__init__.py b/utils/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/utils/file_util.py b/utils/file_util.py
new file mode 100644
index 0000000..366fee7
--- /dev/null
+++ b/utils/file_util.py
@@ -0,0 +1,124 @@
+import os
+
+
+class FileUtil:
+    """
+    Utility class for file operations related to patching Azure Linux packages.
+    """
+
+    def __init__(self):
+        self.base_dir = "/tmp/backport_candidates"
+
+    def create_dir_structure(self, azure_linux_version, package_name, cve_list):
+        """
+        Create the directory structure for the patches.
+        For eg. /tmp/3.0/package_name/cve-id/
+        """
+        base_dir = f"{self.base_dir}/{azure_linux_version}/{package_name}"
+        if not os.path.exists(base_dir):
+            os.makedirs(base_dir)
+
+        for cve in cve_list:
+            cve_dir = os.path.join(base_dir, cve)
+            if not os.path.exists(cve_dir):
+                os.makedirs(cve_dir)
+            else:
+                print(f"Directory {cve_dir} already exists. Skipping creation.")
+
+            # Add a status.txt file containing text "FAIL"
+            status_file_path = os.path.join(cve_dir, "status.txt")
+            if not os.path.exists(status_file_path):
+                with open(status_file_path, "w") as status_file:
+                    status_file.write("FAIL")
+
+    def get_status(self, azure_linux_version, package_name, cve_id):
+        """
+        Get the status of a patch.
+        For eg. /tmp/3.0/package_name/cve-id/status.txt
+        """
+        status_file_path = self.get_aux_directory(
+            azure_linux_version, package_name, cve_id
+        )
+        status_file_path = os.path.join(status_file_path, "status.txt")
+        if not os.path.exists(status_file_path):
+            return "FAIL"
+        with open(status_file_path, "r") as status_file:
+            return status_file.read().strip()
+
+    def set_status(self, azure_linux_version, package_name, cve_id, status):
+        """
+        Mark the status of a patch.
+        For eg. /tmp/3.0/package_name/cve-id/status.txt
+        """
+        status_file_path = self.get_aux_directory(
+            azure_linux_version, package_name, cve_id
+        )
+        status_file_path = os.path.join(status_file_path, "status.txt")
+        with open(status_file_path, "w") as status_file:
+            status_file.write(status)
+
+    def get_fixed_by(self, azure_linux_version, package_name, cve_id):
+        """
+        Get the fixed by information from fixed_by.txt file (autosec or backport)
+        For eg. /tmp/3.0/package_name/cve-id/fixed_by.txt
+        """
+        fixed_by_file_path = self.get_aux_directory(
+            azure_linux_version, package_name, cve_id
+        )
+
+        fixed_by_file_path = os.path.join(fixed_by_file_path, "fixed_by.txt")
+        if not os.path.exists(fixed_by_file_path):
+            return None
+        with open(fixed_by_file_path, "r") as fixed_by_file:
+            return fixed_by_file.read().strip()
+
+    def set_fixed_by(self, azure_linux_version, package_name, cve_id, fixed_by):
+        """
+        Mark the fixed by information on fixed_by.txt file (autosec or backport)
+        For eg. /tmp/3.0/package_name/cve-id/fixed_by.txt
+        """
+        fixed_by_file_path = self.get_aux_directory(
+            azure_linux_version, package_name, cve_id
+        )
+
+        fixed_by_file_path = os.path.join(fixed_by_file_path, "fixed_by.txt")
+        with open(fixed_by_file_path, "w") as fixed_by_file:
+            fixed_by_file.write(fixed_by)
+
+    def get_suffix_path(self, azure_linux_version, package_name, cve_id):
+        """
+        Get the suffix path for the patches.
+        For eg. /3.0/package_name/cve-id/
+        """
+        return f"{azure_linux_version}/{package_name}/{cve_id}"
+
+    def get_aux_directory(self, azure_linux_version, package_name, cve_id):
+        """
+        Get the directory structure for the patches.
+        For eg. /tmp/3.0/package_name/cve-id/
+        """
+        suffix_path = self.get_suffix_path(azure_linux_version, package_name, cve_id)
+        return os.path.join(self.base_dir, suffix_path)
+
+    def iter_directory_structure(self):
+        """
+        Iterate through the directory structure and yield (azure_linux_version, package_name, cve_id, path_to_cve_dir)
+        """
+        if not os.path.exists(self.base_dir):
+            print(f"Base directory {self.base_dir} does not exist.")
+            return
+        for azure_linux_version in os.listdir(self.base_dir):
+            version_path = os.path.join(self.base_dir, azure_linux_version)
+            if not os.path.isdir(version_path):
+                continue
+
+            for package_name in os.listdir(version_path):
+                package_path = os.path.join(version_path, package_name)
+                if not os.path.isdir(package_path):
+                    continue
+
+                for cve_id in os.listdir(package_path):
+                    cve_path = os.path.join(package_path, cve_id)
+                    if not os.path.isdir(cve_path):
+                        continue
+                    yield azure_linux_version, package_name, cve_id, cve_path
-- 
2.45.3

