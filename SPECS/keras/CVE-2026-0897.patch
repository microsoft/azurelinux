From 557b7c38f2797aca0059deb3fafbfea550a093d2 Mon Sep 17 00:00:00 2001
From: AllSpark <allspark@microsoft.com>
Date: Fri, 16 Jan 2026 17:42:25 +0000
Subject: [PATCH] Fix DoS via malicious HDF5 dataset metadata in
 H5IOStore.__getitem__; add MAX_BYTES limit; harden dataset shape/dtype
 validation

Signed-off-by: Azure Linux Security Servicing Account <azurelinux-security@microsoft.com>
Upstream-reference: AI Backport of https://github.com/keras-team/keras/commit/7360d4f0d764fbb1fa9c6408fe53da41974dd4f6.patch
---
 keras/src/saving/saving_lib.py | 59 ++++++++++++++++++++++++++++++++--
 1 file changed, 57 insertions(+), 2 deletions(-)

diff --git a/keras/src/saving/saving_lib.py b/keras/src/saving/saving_lib.py
index 1668489..0bcce01 100644
--- a/keras/src/saving/saving_lib.py
+++ b/keras/src/saving/saving_lib.py
@@ -24,6 +24,10 @@ try:
 except ImportError:
     h5py = None
 
+
+# Maximum allowed HDF5 dataset size in bytes (4 GiB)
+MAX_BYTES = 1 << 32  # 4 GiB
+
 _CONFIG_FILENAME = "config.json"
 _METADATA_FILENAME = "metadata.json"
 _VARS_FNAME = "model.weights"  # Will become e.g. "model.weights.h5"
@@ -696,9 +700,60 @@ class H5Entry:
 
     def __getitem__(self, name):
         value = self.group[name]
+
+        # ------------------------------------------------------
+        # CASE 2 — HDF5 DATASET → SAFE LOADING
+        # ------------------------------------------------------
+
+        # Skip any objects that are not proper datasets
+        if not hasattr(value, "shape") or not hasattr(value, "dtype"):
+            # Fallback: attempt read if possible, else return as-is
+            try:
+                return value[()]
+            except Exception:
+                return value
+
+        shape = value.shape
+        dtype = value.dtype
+
+        # ------------------------------------------------------
+        # Validate SHAPE (avoid malformed / malicious metadata)
+        # ------------------------------------------------------
+
+        # No negative dimensions
+        if any(dim < 0 for dim in shape):
+            raise ValueError(
+                "Malformed HDF5 dataset shape encountered in .keras file; "
+                "negative dimension detected."
+            )
+
+        # Prevent absurdly high-rank tensors
+        if len(shape) > 64:
+            raise ValueError(
+                "Malformed HDF5 dataset shape encountered in .keras file; "
+                "tensor rank exceeds safety limit."
+            )
+
+        # Safe product computation (Python int is unbounded)
+        num_elems = int(np.prod(shape))
+
+        # ------------------------------------------------------
+        # Validate TOTAL memory size
+        # ------------------------------------------------------
+        size_bytes = num_elems * dtype.itemsize
+        if size_bytes > MAX_BYTES:
+            raise ValueError(
+                f"HDF5 dataset too large to load safely "
+                f"({size_bytes} bytes; limit is {MAX_BYTES})."
+            )
+
+        # ------------------------------------------------------
+        # SAFE — load dataset (guaranteed ≤ 4 GiB)
+        # ------------------------------------------------------
+        arr = value[()]
         if "dtype" in value.attrs and value.attrs["dtype"] == "bfloat16":
-            value = np.array(value, dtype=ml_dtypes.bfloat16)
-        return value
+            arr = np.array(arr, dtype=ml_dtypes.bfloat16)
+        return arr
 
 
 class NpzIOStore:
-- 
2.45.4

